# -*- coding: utf-8 -*-
"""
ç®€å•å‘é‡ç¼–ç å™¨ - å°†ä¸»é¢˜è¯è½¬æ¢ä¸ºå‘é‡
ç”¨äºè¯†åº•æ·±æ¹–è®°å¿†ç³»ç»Ÿçš„å‘é‡æ•°æ®åº“åŠŸèƒ½
"""

import re
import json
import os
import math
from collections import Counter
from typing import List, Dict, Optional

class SimpleVectorEncoder:
    """ç®€å•çš„ä¸»é¢˜è¯å‘é‡ç¼–ç å™¨"""
    
    def __init__(self, vocab_file="topic_vocab.json", vector_dim=128):
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„æ–‡ä»¶ç»“æ„
        new_vocab_file = os.path.join("chat_logs", "vectors", "topic_vocab.json")
        if os.path.exists(new_vocab_file):
            self.vocab_file = new_vocab_file
        else:
            self.vocab_file = vocab_file
            
        self.vector_dim = vector_dim
        self.vocab = {}  # è¯æ±‡è¡¨ {word: index}
        self.word_freq = Counter()  # è¯é¢‘ç»Ÿè®¡
        self.idf_scores = {}  # IDFåˆ†æ•°
        self.load_vocab()
    
    def load_vocab(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        if os.path.exists(self.vocab_file):
            try:
                with open(self.vocab_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.vocab = data.get("vocab", {})
                    self.word_freq = Counter(data.get("word_freq", {}))
                    self.idf_scores = data.get("idf_scores", {})
                    print(f"ğŸ“š åŠ è½½è¯æ±‡è¡¨: {len(self.vocab)} ä¸ªè¯æ±‡")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
                self.vocab = {}
                self.word_freq = Counter()
                self.idf_scores = {}
    
    def save_vocab(self):
        """ä¿å­˜è¯æ±‡è¡¨"""
        try:
            data = {
                "vocab": self.vocab,
                "word_freq": dict(self.word_freq),
                "idf_scores": self.idf_scores
            }
            with open(self.vocab_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯æ±‡è¡¨å¤±è´¥: {e}")
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯ - ç®€å•çš„ä¸­æ–‡åˆ†è¯"""
        if not text:
            return []
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†ç¦»ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å•è¯
        tokens = []
        current_word = ""
        
        for char in text:
            if char.isalpha():
                if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                    if current_word:
                        tokens.append(current_word.lower())
                        current_word = ""
                    tokens.append(char)
                else:  # è‹±æ–‡å­—ç¬¦
                    current_word += char
            elif char.isspace():
                if current_word:
                    tokens.append(current_word.lower())
                    current_word = ""
            else:
                if current_word:
                    tokens.append(current_word.lower())
                    current_word = ""
        
        if current_word:
            tokens.append(current_word.lower())
        
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
        tokens = [token for token in tokens if len(token) >= 1]
        
        return tokens
    
    def update_vocab(self, texts: List[str]):
        """æ›´æ–°è¯æ±‡è¡¨"""
        all_tokens = []
        doc_count = {}  # æ¯ä¸ªè¯å‡ºç°åœ¨å¤šå°‘ä¸ªæ–‡æ¡£ä¸­
        
        for text in texts:
            tokens = self.tokenize(text)
            all_tokens.extend(tokens)
            
            # ç»Ÿè®¡æ–‡æ¡£é¢‘ç‡
            unique_tokens = set(tokens)
            for token in unique_tokens:
                doc_count[token] = doc_count.get(token, 0) + 1
        
        # æ›´æ–°è¯é¢‘
        self.word_freq.update(all_tokens)
        
        # é‡å»ºè¯æ±‡è¡¨ç´¢å¼•
        unique_words = list(self.word_freq.keys())
        self.vocab = {word: idx for idx, word in enumerate(unique_words)}
        
        # è®¡ç®—IDFåˆ†æ•°
        total_docs = len(texts) if texts else 1
        for word, df in doc_count.items():
            self.idf_scores[word] = math.log(total_docs / (df + 1))
        
        print(f"ğŸ“š æ›´æ–°è¯æ±‡è¡¨: {len(self.vocab)} ä¸ªè¯æ±‡")
        self.save_vocab()
    
    def encode_text(self, text: str) -> Optional[List[float]]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        if not text:
            return None
        
        tokens = self.tokenize(text)
        if not tokens:
            return None
        
        # åˆ›å»ºTF-IDFå‘é‡
        vector = [0.0] * self.vector_dim
        token_count = Counter(tokens)
        
        for token, tf in token_count.items():
            if token in self.vocab:
                idx = self.vocab[token] % self.vector_dim  # æ˜ å°„åˆ°å‘é‡ç»´åº¦
                idf = self.idf_scores.get(token, 1.0)
                tfidf = tf * idf
                vector[idx] += tfidf
        
        # å½’ä¸€åŒ–å‘é‡
        vector_norm = math.sqrt(sum(x * x for x in vector))
        if vector_norm > 0:
            vector = [x / vector_norm for x in vector]
        
        return vector
    
    def calculate_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vector1 or not vector2:
            return 0.0
        
        if len(vector1) != len(vector2):
            return 0.0
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(a * b for a, b in zip(vector1, vector2))
        norm1 = math.sqrt(sum(a * a for a in vector1))
        norm2 = math.sqrt(sum(b * b for b in vector2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def get_stats(self) -> Dict:
        """è·å–ç¼–ç å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "vocab_size": len(self.vocab),
            "total_words": sum(self.word_freq.values()),
            "unique_words": len(self.word_freq),
            "vector_dim": self.vector_dim
        }

# å…¨å±€å®ä¾‹
_encoder_instance = None

def get_vector_encoder() -> SimpleVectorEncoder:
    """è·å–å‘é‡ç¼–ç å™¨å®ä¾‹"""
    global _encoder_instance
    if _encoder_instance is None:
        _encoder_instance = SimpleVectorEncoder()
    return _encoder_instance
