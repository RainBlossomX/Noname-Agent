#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡ä»¶åˆ†æå·¥å…· - éœ²å°¼è¥¿äºšçš„æ–‡ä»¶å¤„ç†èƒ½åŠ›
æ”¯æŒPDFã€CSVã€Excelç­‰æ–‡ä»¶ç±»å‹çš„æ™ºèƒ½åˆ†æ
ä½¿ç”¨LangChainä½œä¸ºç»Ÿä¸€æ¥å£ï¼ŒPyMuPDFå¤„ç†PDFï¼Œpandaså¤„ç†è¡¨æ ¼
"""

import os
import json
import fitz  # PyMuPDF
import pandas as pd
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
import io
import base64
from pathlib import Path
import sys

# å®‰å…¨æ‰“å°å‡½æ•°ï¼ˆé¿å…Windowsç»ˆç«¯emojiç¼–ç é—®é¢˜ï¼‰
def safe_print(msg):
    """å®‰å…¨æ‰“å°ï¼Œé¿å…emojiç¼–ç é”™è¯¯"""
    try:
        print(msg)
    except UnicodeEncodeError:
        # ç§»é™¤æ‰€æœ‰éASCIIå’Œå¸¸è§ä¸­æ–‡ä¹‹å¤–çš„å­—ç¬¦ï¼ˆåŒ…æ‹¬emojiï¼‰
        import re
        # åªä¿ç•™ASCIIã€ä¸­æ–‡ã€å¸¸è§æ ‡ç‚¹
        msg_safe = re.sub(r'[^\x00-\x7F\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', '', msg)
        try:
            print(msg_safe)
        except UnicodeEncodeError:
            # æœ€ç»ˆå›é€€ï¼šåªä¿ç•™ASCII
            msg_ascii = re.sub(r'[^\x00-\x7F]', '', msg)
            print(msg_ascii)

# python-docx for Word documents
try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    safe_print("âš ï¸ python-docx æœªå®‰è£…ï¼ŒWordæ–‡æ¡£åˆ†æåŠŸèƒ½ä¸å¯ç”¨")

# Code analyzer for programming languages
from code_analyzer import PythonCodeAnalyzer, GeneralCodeAnalyzer, CodeAnalysisResult

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.document_loaders.base import BaseLoader

@dataclass
class FileAnalysisResult:
    """æ–‡ä»¶åˆ†æç»“æœ"""
    file_type: str
    file_name: str
    content: str
    metadata: Dict[str, Any]
    summary: str
    analysis: str
    success: bool
    error: Optional[str] = None

class PDFAnalyzer:
    """PDFæ–‡ä»¶åˆ†æå™¨"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
    
    def extract_text(self, file_path: str) -> FileAnalysisResult:
        """æå–PDFæ–‡æœ¬å†…å®¹"""
        try:
            safe_print(f"ğŸ“„ å¼€å§‹åˆ†æPDFæ–‡ä»¶: {file_path}")
            
            # ä½¿ç”¨PyMuPDFæ‰“å¼€PDF
            doc = fitz.open(file_path)
            text_content = ""
            metadata = {
                "page_count": doc.page_count,
                "title": doc.metadata.get("title", ""),
                "author": doc.metadata.get("author", ""),
                "subject": doc.metadata.get("subject", ""),
                "creator": doc.metadata.get("creator", ""),
                "producer": doc.metadata.get("producer", ""),
                "creation_date": doc.metadata.get("creationDate", ""),
                "modification_date": doc.metadata.get("modDate", ""),
            }
            
            # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
            for page_num in range(doc.page_count):
                page = doc[page_num]
                page_text = page.get_text()
                text_content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}"
            
            doc.close()
            
            # ä½¿ç”¨LangChainåˆ†å‰²æ–‡æœ¬
            documents = self.text_splitter.split_text(text_content)
            
            # ç”Ÿæˆæ‘˜è¦å’Œåˆ†æ
            summary = self._generate_summary(text_content, metadata)
            analysis = self._analyze_content(text_content, metadata)
            
            return FileAnalysisResult(
                file_type="PDF",
                file_name=os.path.basename(file_path),
                content=text_content,
                metadata=metadata,
                summary=summary,
                analysis=analysis,
                success=True
            )
            
        except Exception as e:
            safe_print(f"âŒ PDFåˆ†æå¤±è´¥: {e}")
            return FileAnalysisResult(
                file_type="PDF",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error=str(e)
            )
    
    def _generate_summary(self, content: str, metadata: Dict) -> str:
        """ç”ŸæˆPDFæ‘˜è¦"""
        lines = content.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        
        summary_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        if metadata.get("title"):
            summary_parts.append(f"ğŸ“– æ ‡é¢˜: {metadata['title']}")
        if metadata.get("author"):
            summary_parts.append(f"ğŸ‘¤ ä½œè€…: {metadata['author']}")
        if metadata.get("page_count"):
            summary_parts.append(f"ğŸ“„ é¡µæ•°: {metadata['page_count']}")
        
        # å†…å®¹æ¦‚è§ˆ
        summary_parts.append(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        summary_parts.append(f"ğŸ“Š æœ‰æ•ˆè¡Œæ•°: {len(non_empty_lines)} è¡Œ")
        
        # å…³é”®è¯æå–ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        words = content.lower().split()
        word_freq = {}
        for word in words:
            if len(word) > 3:  # åªç»Ÿè®¡é•¿åº¦å¤§äº3çš„è¯
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # è·å–é«˜é¢‘è¯
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        if top_words:
            summary_parts.append(f"ğŸ”‘ å…³é”®è¯: {', '.join([word for word, freq in top_words])}")
        
        return "\n".join(summary_parts)
    
    def _analyze_content(self, content: str, metadata: Dict) -> str:
        """åˆ†æPDFå†…å®¹"""
        analysis_parts = []
        
        # æ–‡æ¡£ç»“æ„åˆ†æ
        lines = content.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        
        # æ£€æµ‹å¯èƒ½çš„ç« èŠ‚
        chapter_indicators = ['ç¬¬', 'ç« ', 'Chapter', 'Section', 'éƒ¨åˆ†']
        chapters = [line for line in non_empty_lines if any(indicator in line for indicator in chapter_indicators)]
        
        if chapters:
            analysis_parts.append(f"ğŸ“š æ£€æµ‹åˆ° {len(chapters)} ä¸ªå¯èƒ½çš„ç« èŠ‚æ ‡é¢˜")
        
        # æ£€æµ‹è¡¨æ ¼ï¼ˆç®€å•æ£€æµ‹ï¼‰
        table_indicators = ['|', '\t', '  ']
        table_lines = [line for line in non_empty_lines if any(indicator in line for indicator in table_indicators)]
        if len(table_lines) > 5:
            analysis_parts.append("ğŸ“Š æ£€æµ‹åˆ°å¯èƒ½çš„è¡¨æ ¼æ•°æ®")
        
        # æ£€æµ‹åˆ—è¡¨
        list_indicators = ['â€¢', '-', '*', '1.', '2.', '3.']
        list_lines = [line for line in non_empty_lines if any(line.strip().startswith(indicator) for indicator in list_indicators)]
        if list_lines:
            analysis_parts.append(f"ğŸ“‹ æ£€æµ‹åˆ° {len(list_lines)} ä¸ªåˆ—è¡¨é¡¹")
        
        # æ£€æµ‹æ•°å­—å’Œç»Ÿè®¡ä¿¡æ¯
        import re
        numbers = re.findall(r'\d+', content)
        if len(numbers) > 10:
            analysis_parts.append("ğŸ“ˆ åŒ…å«å¤§é‡æ•°å­—æ•°æ®ï¼Œå¯èƒ½æ˜¯ç»Ÿè®¡æŠ¥å‘Š")
        
        return "\n".join(analysis_parts) if analysis_parts else "ğŸ“„ æ ‡å‡†æ–‡æ¡£å†…å®¹"

class TableAnalyzer:
    """è¡¨æ ¼æ–‡ä»¶åˆ†æå™¨ï¼ˆCSVã€Excelï¼‰"""
    
    def __init__(self):
        self.supported_formats = ['.csv', '.xlsx', '.xls']
    
    def analyze_table(self, file_path: str) -> FileAnalysisResult:
        """åˆ†æè¡¨æ ¼æ–‡ä»¶"""
        try:
            safe_print(f"ğŸ“Š å¼€å§‹åˆ†æè¡¨æ ¼æ–‡ä»¶: {file_path}")
            
            file_ext = os.path.splitext(file_path)[1].lower()
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ•°æ®
            if file_ext == '.csv':
                df = pd.read_csv(file_path, encoding='utf-8')
            elif file_ext in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = {
                "file_type": file_ext,
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": df.columns.tolist(),
                "data_types": df.dtypes.to_dict(),
                "memory_usage": df.memory_usage(deep=True).sum(),
            }
            
            # ç”Ÿæˆå†…å®¹æ‘˜è¦
            content = self._generate_table_content(df)
            summary = self._generate_table_summary(df, metadata)
            analysis = self._analyze_table_data(df, metadata)
            
            return FileAnalysisResult(
                file_type="TABLE",
                file_name=os.path.basename(file_path),
                content=content,
                metadata=metadata,
                summary=summary,
                analysis=analysis,
                success=True
            )
            
        except Exception as e:
            safe_print(f"âŒ è¡¨æ ¼åˆ†æå¤±è´¥: {e}")
            return FileAnalysisResult(
                file_type="TABLE",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error=str(e)
            )
    
    def _generate_table_content(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆè¡¨æ ¼å†…å®¹æ–‡æœ¬"""
        content_parts = []
        
        # æ·»åŠ åˆ—å
        content_parts.append("åˆ—å: " + ", ".join(df.columns.tolist()))
        content_parts.append("")
        
        # æ·»åŠ å‰å‡ è¡Œæ•°æ®ä½œä¸ºç¤ºä¾‹
        content_parts.append("æ•°æ®é¢„è§ˆ:")
        content_parts.append(df.head(10).to_string())
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if not df.empty:
            content_parts.append("\nç»Ÿè®¡ä¿¡æ¯:")
            content_parts.append(df.describe().to_string())
        
        return "\n".join(content_parts)
    
    def _generate_table_summary(self, df: pd.DataFrame, metadata: Dict) -> str:
        """ç”Ÿæˆè¡¨æ ¼æ‘˜è¦"""
        summary_parts = []
        
        summary_parts.append(f"ğŸ“Š æ•°æ®ç»´åº¦: {metadata['rows']} è¡Œ Ã— {metadata['columns']} åˆ—")
        summary_parts.append(f"ğŸ“ åˆ—å: {', '.join(metadata['column_names'])}")
        
        # æ•°æ®ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for dtype in df.dtypes:
            type_name = str(dtype)
            type_counts[type_name] = type_counts.get(type_name, 0) + 1
        
        type_info = ", ".join([f"{dtype}({count})" for dtype, count in type_counts.items()])
        summary_parts.append(f"ğŸ”¢ æ•°æ®ç±»å‹: {type_info}")
        
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing_values = df.isnull().sum()
        if missing_values.sum() > 0:
            missing_info = ", ".join([f"{col}({count})" for col, count in missing_values.items() if count > 0])
            summary_parts.append(f"âš ï¸ ç¼ºå¤±å€¼: {missing_info}")
        else:
            summary_parts.append("âœ… æ— ç¼ºå¤±å€¼")
        
        return "\n".join(summary_parts)
    
    def _analyze_table_data(self, df: pd.DataFrame, metadata: Dict) -> str:
        """åˆ†æè¡¨æ ¼æ•°æ®"""
        analysis_parts = []
        
        # æ•°å€¼åˆ—åˆ†æ
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            analysis_parts.append(f"ğŸ“ˆ æ•°å€¼åˆ—: {len(numeric_cols)} ä¸ª")
            
            # æ£€æµ‹å¼‚å¸¸å€¼
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
                if len(outliers) > 0:
                    analysis_parts.append(f"âš ï¸ {col}åˆ—æœ‰{len(outliers)}ä¸ªå¼‚å¸¸å€¼")
        
        # æ–‡æœ¬åˆ—åˆ†æ
        text_cols = df.select_dtypes(include=['object']).columns
        if len(text_cols) > 0:
            analysis_parts.append(f"ğŸ“ æ–‡æœ¬åˆ—: {len(text_cols)} ä¸ª")
            
            # æ£€æµ‹é‡å¤å€¼
            for col in text_cols:
                duplicates = df[col].duplicated().sum()
                if duplicates > 0:
                    analysis_parts.append(f"ğŸ”„ {col}åˆ—æœ‰{duplicates}ä¸ªé‡å¤å€¼")
        
        # æ£€æµ‹å¯èƒ½çš„IDåˆ—
        id_candidates = []
        for col in df.columns:
            if df[col].nunique() == len(df) and df[col].dtype == 'object':
                id_candidates.append(col)
        
        if id_candidates:
            analysis_parts.append(f"ğŸ†” å¯èƒ½çš„IDåˆ—: {', '.join(id_candidates)}")
        
        # æ£€æµ‹æ—¶é—´åˆ—
        time_candidates = []
        for col in df.columns:
            if 'date' in col.lower() or 'time' in col.lower():
                time_candidates.append(col)
        
        if time_candidates:
            analysis_parts.append(f"â° å¯èƒ½çš„æ—¶é—´åˆ—: {', '.join(time_candidates)}")
        
        return "\n".join(analysis_parts) if analysis_parts else "ğŸ“Š æ ‡å‡†è¡¨æ ¼æ•°æ®"

class DocxAnalyzer:
    """Wordæ–‡æ¡£åˆ†æå™¨ (ä½¿ç”¨python-docx)"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
    
    def extract_text(self, file_path: str) -> FileAnalysisResult:
        """æå–Wordæ–‡æ¡£æ–‡æœ¬å†…å®¹"""
        if not DOCX_AVAILABLE:
            return FileAnalysisResult(
                file_type="DOCX",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error="python-docxæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-docx"
            )
        
        try:
            safe_print(f"ğŸ“ å¼€å§‹åˆ†æWordæ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨python-docxæ‰“å¼€Wordæ–‡æ¡£
            doc = DocxDocument(file_path)
            
            # æå–å…ƒæ•°æ®
            core_props = doc.core_properties
            metadata = {
                "title": core_props.title or "",
                "author": core_props.author or "",
                "subject": core_props.subject or "",
                "keywords": core_props.keywords or "",
                "created": str(core_props.created) if core_props.created else "",
                "modified": str(core_props.modified) if core_props.modified else "",
                "last_modified_by": core_props.last_modified_by or "",
                "paragraph_count": len(doc.paragraphs),
                "table_count": len(doc.tables),
            }
            
            # æå–æ®µè½æ–‡æœ¬
            text_content = ""
            paragraph_count = 0
            
            for para in doc.paragraphs:
                para_text = para.text.strip()
                if para_text:  # åªæ·»åŠ éç©ºæ®µè½
                    text_content += para_text + "\n\n"
                    paragraph_count += 1
            
            # æå–è¡¨æ ¼å†…å®¹
            table_content = ""
            for table_idx, table in enumerate(doc.tables):
                table_content += f"\n--- è¡¨æ ¼ {table_idx + 1} ---\n"
                for row_idx, row in enumerate(table.rows):
                    row_data = [cell.text.strip() for cell in row.cells]
                    table_content += " | ".join(row_data) + "\n"
                table_content += "\n"
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹
            full_content = text_content
            if table_content:
                full_content += "\nğŸ“Š æ–‡æ¡£ä¸­çš„è¡¨æ ¼:\n" + table_content
            
            metadata["has_tables"] = len(doc.tables) > 0
            metadata["has_images"] = self._count_images(doc)
            
            # ç”Ÿæˆæ‘˜è¦å’Œåˆ†æ
            summary = self._generate_summary(text_content, metadata)
            analysis = self._analyze_content(text_content, metadata, doc)
            
            return FileAnalysisResult(
                file_type="DOCX",
                file_name=os.path.basename(file_path),
                content=full_content,
                metadata=metadata,
                summary=summary,
                analysis=analysis,
                success=True
            )
            
        except Exception as e:
            safe_print(f"âŒ Wordæ–‡æ¡£åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return FileAnalysisResult(
                file_type="DOCX",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error=str(e)
            )
    
    def _count_images(self, doc) -> int:
        """ç»Ÿè®¡æ–‡æ¡£ä¸­çš„å›¾ç‰‡æ•°é‡"""
        try:
            image_count = 0
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    image_count += 1
            return image_count
        except:
            return 0
    
    def _generate_summary(self, content: str, metadata: Dict) -> str:
        """ç”ŸæˆWordæ–‡æ¡£æ‘˜è¦"""
        summary_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        if metadata.get("title"):
            summary_parts.append(f"ğŸ“– æ ‡é¢˜: {metadata['title']}")
        if metadata.get("author"):
            summary_parts.append(f"ğŸ‘¤ ä½œè€…: {metadata['author']}")
        if metadata.get("subject"):
            summary_parts.append(f"ğŸ“Œ ä¸»é¢˜: {metadata['subject']}")
        
        # æ–‡æ¡£ç»“æ„
        summary_parts.append(f"ğŸ“„ æ®µè½æ•°: {metadata.get('paragraph_count', 0)}")
        if metadata.get("table_count", 0) > 0:
            summary_parts.append(f"ğŸ“Š è¡¨æ ¼æ•°: {metadata['table_count']}")
        if metadata.get("has_images"):
            summary_parts.append(f"ğŸ–¼ï¸ åŒ…å«å›¾ç‰‡")
        
        # å†…å®¹ç»Ÿè®¡
        summary_parts.append(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # å…³é”®è¯
        if metadata.get("keywords"):
            summary_parts.append(f"ğŸ”‘ å…³é”®è¯: {metadata['keywords']}")
        
        # æ—¶é—´ä¿¡æ¯
        if metadata.get("created"):
            summary_parts.append(f"ğŸ“… åˆ›å»ºæ—¶é—´: {metadata['created']}")
        if metadata.get("modified"):
            summary_parts.append(f"ğŸ”„ ä¿®æ”¹æ—¶é—´: {metadata['modified']}")
        
        return "\n".join(summary_parts)
    
    def _analyze_content(self, content: str, metadata: Dict, doc) -> str:
        """åˆ†æWordæ–‡æ¡£å†…å®¹"""
        analysis_parts = []
        
        # æ–‡æ¡£ç»“æ„åˆ†æ
        para_count = metadata.get("paragraph_count", 0)
        table_count = metadata.get("table_count", 0)
        
        if para_count > 50:
            analysis_parts.append("ğŸ“š é•¿ç¯‡æ–‡æ¡£ï¼Œå†…å®¹ä¸°å¯Œ")
        elif para_count > 20:
            analysis_parts.append("ğŸ“„ ä¸­ç­‰é•¿åº¦æ–‡æ¡£")
        else:
            analysis_parts.append("ğŸ“‹ ç®€çŸ­æ–‡æ¡£")
        
        # è¡¨æ ¼åˆ†æ
        if table_count > 0:
            analysis_parts.append(f"ğŸ“Š åŒ…å« {table_count} ä¸ªè¡¨æ ¼ï¼Œå¯èƒ½åŒ…å«ç»“æ„åŒ–æ•°æ®")
        
        # å›¾ç‰‡åˆ†æ
        if metadata.get("has_images"):
            analysis_parts.append("ğŸ–¼ï¸ åŒ…å«å›¾ç‰‡ï¼Œå¯èƒ½æ˜¯å›¾æ–‡æ··æ’æ–‡æ¡£")
        
        # æ ‡é¢˜åˆ†æï¼ˆæ£€æµ‹æ ·å¼ï¼‰
        heading_count = sum(1 for para in doc.paragraphs if para.style.name.startswith('Heading'))
        if heading_count > 0:
            analysis_parts.append(f"ğŸ“‘ æ£€æµ‹åˆ° {heading_count} ä¸ªæ ‡é¢˜ï¼Œæ–‡æ¡£ç»“æ„æ¸…æ™°")
        
        # åˆ—è¡¨åˆ†æ
        list_count = sum(1 for para in doc.paragraphs if 'List' in para.style.name)
        if list_count > 5:
            analysis_parts.append(f"ğŸ“‹ åŒ…å« {list_count} ä¸ªåˆ—è¡¨é¡¹")
        
        # æ£€æµ‹æ•°å­—å’Œç»Ÿè®¡ä¿¡æ¯
        import re
        numbers = re.findall(r'\d+', content)
        if len(numbers) > 20:
            analysis_parts.append("ğŸ“ˆ åŒ…å«å¤§é‡æ•°å­—æ•°æ®ï¼Œå¯èƒ½æ˜¯æŠ¥å‘Šæˆ–åˆ†ææ–‡æ¡£")
        
        # æ£€æµ‹æ—¶é—´ä¿¡æ¯
        years = re.findall(r'\b(19|20)\d{2}\b', content)
        if years:
            unique_years = sorted(set(years))
            if len(unique_years) > 1:
                analysis_parts.append(f"â° æ¶‰åŠæ—¶é—´èŒƒå›´: {unique_years[0]}-{unique_years[-1]}")
        
        # æ£€æµ‹URLå’Œé“¾æ¥
        urls = re.findall(r'https?://[^\s]+', content)
        if urls:
            analysis_parts.append(f"ğŸ”— åŒ…å« {len(urls)} ä¸ªé“¾æ¥")
        
        # æ£€æµ‹é‚®ç®±
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content)
        if emails:
            analysis_parts.append(f"ğŸ“§ åŒ…å« {len(emails)} ä¸ªé‚®ç®±åœ°å€")
        
        return "\n".join(analysis_parts) if analysis_parts else "ğŸ“„ æ ‡å‡†Wordæ–‡æ¡£"

class FileAnalysisTool:
    """æ–‡ä»¶åˆ†æå·¥å…·ä¸»ç±»"""
    
    def __init__(self, config=None):
        self.pdf_analyzer = PDFAnalyzer()
        self.table_analyzer = TableAnalyzer()
        self.docx_analyzer = DocxAnalyzer()
        self.python_analyzer = PythonCodeAnalyzer()
        self.general_code_analyzer = GeneralCodeAnalyzer()
        self.supported_types = {
            '.pdf': 'PDFæ–‡æ¡£',
            '.csv': 'CSVè¡¨æ ¼',
            '.xlsx': 'Excelè¡¨æ ¼',
            '.xls': 'Excelè¡¨æ ¼',
            '.docx': 'Wordæ–‡æ¡£',
            '.doc': 'Wordæ–‡æ¡£ï¼ˆæ—§ç‰ˆï¼‰',
            '.py': 'Pythonä»£ç ',
            '.java': 'Javaä»£ç ',
            '.js': 'JavaScriptä»£ç ',
            '.jsx': 'Reactä»£ç ',
            '.ts': 'TypeScriptä»£ç ',
            '.tsx': 'TypeScript Reactä»£ç ',
            '.cpp': 'C++ä»£ç ',
            '.c': 'Cä»£ç ',
            '.h': 'C/C++å¤´æ–‡ä»¶',
            '.hpp': 'C++å¤´æ–‡ä»¶',
            '.go': 'Goä»£ç ',
            '.rs': 'Rustä»£ç '
        }
        
        # åˆå§‹åŒ–AI Agent
        if config:
            from file_analysis_agent import FileAnalysisAgent
            self.ai_agent = FileAnalysisAgent(config)
        else:
            self.ai_agent = None
    
    def analyze_file(self, file_path: str) -> FileAnalysisResult:
        """åˆ†ææ–‡ä»¶"""
        if not os.path.exists(file_path):
            return FileAnalysisResult(
                file_type="UNKNOWN",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error="æ–‡ä»¶ä¸å­˜åœ¨"
            )
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        safe_print(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_path} (ç±»å‹: {file_ext})")
        
        if file_ext == '.pdf':
            return self.pdf_analyzer.extract_text(file_path)
        elif file_ext in ['.csv', '.xlsx', '.xls']:
            return self.table_analyzer.analyze_table(file_path)
        elif file_ext in ['.docx', '.doc']:
            return self.docx_analyzer.extract_text(file_path)
        elif file_ext == '.py':
            # Pythonä»£ç åˆ†æ
            code_result = self.python_analyzer.analyze(file_path)
            return self._convert_code_result(code_result)
        elif file_ext in ['.java', '.js', '.jsx', '.ts', '.tsx', '.cpp', '.c', '.h', '.hpp', '.go', '.rs']:
            # å…¶ä»–ç¼–ç¨‹è¯­è¨€ä»£ç åˆ†æ
            code_result = self.general_code_analyzer.analyze(file_path)
            return self._convert_code_result(code_result)
        else:
            return FileAnalysisResult(
                file_type="UNSUPPORTED",
                file_name=os.path.basename(file_path),
                content="",
                metadata={},
                summary="",
                analysis="",
                success=False,
                error=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
            )
    
    def _convert_code_result(self, code_result: CodeAnalysisResult) -> FileAnalysisResult:
        """å°†ä»£ç åˆ†æç»“æœè½¬æ¢ä¸ºæ–‡ä»¶åˆ†æç»“æœ"""
        return FileAnalysisResult(
            file_type=f"CODE_{code_result.language.upper()}",
            file_name=code_result.file_name,
            content=code_result.content,
            metadata={
                "language": code_result.language,
                "structure": code_result.structure,
                "metrics": code_result.metrics
            },
            summary=code_result.summary,
            analysis=code_result.analysis,
            success=code_result.success,
            error=code_result.error
        )
    
    def get_supported_types(self) -> Dict[str, str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
        return self.supported_types
    
    def generate_ai_analysis(self, result: FileAnalysisResult, user_question: str = "") -> str:
        """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
        if not result.success:
            return f"âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {result.error}"
        
        analysis_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        analysis_parts.append(f"ğŸ“ **æ–‡ä»¶ä¿¡æ¯**")
        analysis_parts.append(f"- æ–‡ä»¶å: {result.file_name}")
        analysis_parts.append(f"- æ–‡ä»¶ç±»å‹: {result.file_type}")
        analysis_parts.append("")
        
        # æ‘˜è¦
        analysis_parts.append(f"ğŸ“‹ **æ–‡ä»¶æ‘˜è¦**")
        analysis_parts.append(result.summary)
        analysis_parts.append("")
        
        # æ™ºèƒ½åˆ†æ
        analysis_parts.append(f"ğŸ” **æ™ºèƒ½åˆ†æ**")
        analysis_parts.append(result.analysis)
        analysis_parts.append("")
        
        # ä½¿ç”¨ä¸“é—¨çš„AI Agentè¿›è¡Œæ·±åº¦åˆ†æ
        if self.ai_agent:
            try:
                ai_analysis = self.ai_agent.analyze_file_with_ai(result, user_question)
                if ai_analysis:
                    analysis_parts.append(ai_analysis)
            except Exception as e:
                safe_print(f"âš ï¸ AI Agentåˆ†æå¤±è´¥: {e}")
                # å›é€€åˆ°ç®€å•åˆ†æ
                ai_insights = self._generate_ai_insights(result, user_question)
                if ai_insights:
                    analysis_parts.append(f"âœ… **AIæ·±åº¦åˆ†æ**")
                    analysis_parts.append(ai_insights)
        else:
            # å›é€€åˆ°ç®€å•åˆ†æ
            ai_insights = self._generate_ai_insights(result, user_question)
            if ai_insights:
                analysis_parts.append(f"âœ… **AIæ·±åº¦åˆ†æ**")
                analysis_parts.append(ai_insights)
        
        # ä¸å†æ˜¾ç¤ºå…³é”®ä¿¡æ¯å’Œå†…å®¹é¢„è§ˆï¼ˆå·²åŒ…å«åœ¨AIåˆ†ææŠ¥å‘Šä¸­ï¼‰
        
        return "\n".join(analysis_parts)
    
    def _generate_ai_insights(self, result: FileAnalysisResult, user_question: str = "") -> str:
        """ç”ŸæˆAIæ·±åº¦åˆ†ææ´å¯Ÿ"""
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹ç”Ÿæˆä¸åŒçš„AIåˆ†æ
            if result.file_type == "PDF":
                return self._analyze_pdf_with_ai(result, user_question)
            elif result.file_type == "TABLE":
                return self._analyze_table_with_ai(result, user_question)
            else:
                return self._analyze_general_with_ai(result, user_question)
        except Exception as e:
            safe_print(f"âš ï¸ AIåˆ†æç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _analyze_pdf_with_ai(self, result: FileAnalysisResult, user_question: str) -> str:
        """ä½¿ç”¨AIåˆ†æPDFå†…å®¹"""
        insights = []
        
        # æ–‡æ¡£ç»“æ„åˆ†æ
        content = result.content
        lines = content.split('\n')
        
        # æ£€æµ‹ç« èŠ‚ç»“æ„
        chapters = [line.strip() for line in lines if 'ç¬¬' in line and 'ç« ' in line]
        if chapters:
            insights.append(f"ğŸ“š **æ–‡æ¡£ç»“æ„**: æ£€æµ‹åˆ° {len(chapters)} ä¸ªç« èŠ‚")
            insights.append(f"   ä¸»è¦ç« èŠ‚: {', '.join(chapters[:3])}{'...' if len(chapters) > 3 else ''}")
        
        # å…³é”®æ¦‚å¿µæå–
        key_concepts = self._extract_key_concepts(content)
        if key_concepts:
            insights.append(f"ğŸ”‘ **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(key_concepts[:5])}")
        
        # æ•°æ®ç»Ÿè®¡
        numbers = [word for word in content.split() if word.isdigit()]
        if len(numbers) > 5:
            insights.append(f"ğŸ“Š **æ•°æ®ä¸°å¯Œåº¦**: åŒ…å« {len(numbers)} ä¸ªæ•°å­—ï¼Œæ•°æ®è¯¦å®")
        
        # æ—¶é—´çº¿æ£€æµ‹
        time_indicators = ['å¹´', 'å¹´ä»£', 'ä¸–çºª', 'æœˆ', 'æ—¥']
        time_mentions = [line for line in lines if any(indicator in line for indicator in time_indicators)]
        if time_mentions:
            insights.append(f"â° **æ—¶é—´ç»´åº¦**: åŒ…å«å†å²å‘å±•è„‰ç»œ")
        
        # è¡¨æ ¼æ£€æµ‹
        table_indicators = ['|', '\t', '  ']
        table_lines = [line for line in lines if any(indicator in line for indicator in table_indicators)]
        if len(table_lines) > 3:
            insights.append(f"ğŸ“‹ **ç»“æ„åŒ–æ•°æ®**: åŒ…å«è¡¨æ ¼æˆ–åˆ—è¡¨ä¿¡æ¯")
        
        # æ™ºèƒ½æ€»ç»“ç”Ÿæˆ
        smart_summary = self._generate_smart_summary(content, user_question)
        if smart_summary:
            insights.append(f"ğŸ’¡ **æ™ºèƒ½æ€»ç»“**: {smart_summary}")
        
        return "\n".join(insights) if insights else "ğŸ“„ æ ‡å‡†æ–‡æ¡£å†…å®¹ï¼Œç»“æ„æ¸…æ™°"
    
    def _generate_smart_summary(self, content: str, user_question: str = "") -> str:
        """ç”Ÿæˆæ™ºèƒ½æ€»ç»“"""
        try:
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œå…ˆè¿›è¡Œæ–‡æœ¬åˆ†å‰²
            if len(content) > 2000:
                # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                )
                chunks = text_splitter.split_text(content)
                
                # å–å‰å‡ ä¸ªchunkè¿›è¡Œæ€»ç»“
                summary_content = "\n".join(chunks[:3])
            else:
                summary_content = content
            
            # ç”Ÿæˆæ€»ç»“æç¤ºè¯
            if user_question:
                prompt = f"""
è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜"{user_question}"ï¼Œå¯¹ä»¥ä¸‹æ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†æå’Œæ€»ç»“ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{summary_content[:1500]}

è¯·æä¾›ï¼š
1. æ ¸å¿ƒè§‚ç‚¹æ€»ç»“ï¼ˆ2-3ä¸ªè¦ç‚¹ï¼‰
2. å…³é”®æ•°æ®æˆ–äº‹å®
3. ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„é‡ç‚¹å†…å®¹

æ€»ç»“è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹ã€‚
"""
            else:
                prompt = f"""
è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½æ€»ç»“ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{summary_content[:1500]}

è¯·æä¾›ï¼š
1. æ–‡æ¡£ä¸»é¢˜å’Œæ ¸å¿ƒè§‚ç‚¹
2. ä¸»è¦ç« èŠ‚æˆ–ç»“æ„
3. å…³é”®ä¿¡æ¯è¦ç‚¹

æ€»ç»“è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹ã€‚
"""
            
            # è¿™é‡Œå¯ä»¥è°ƒç”¨AI APIï¼Œç°åœ¨å…ˆç”¨ç®€å•çš„æ–‡æœ¬åˆ†æ
            return self._simple_content_analysis(summary_content)
            
        except Exception as e:
            safe_print(f"âš ï¸ æ™ºèƒ½æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _simple_content_analysis(self, content: str) -> str:
        """ç®€å•çš„å†…å®¹åˆ†æï¼ˆä¸ä½¿ç”¨AI APIï¼‰"""
        try:
            lines = content.split('\n')
            
            # æå–æ ‡é¢˜å’Œå…³é”®å¥
            key_sentences = []
            
            # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
            important_keywords = ['äººå·¥æ™ºèƒ½', 'AI', 'æŠ€æœ¯', 'å‘å±•', 'åº”ç”¨', 'æŒ‘æˆ˜', 'æœªæ¥', 'æ™ºèƒ½']
            
            for line in lines:
                line = line.strip()
                if len(line) > 20 and any(keyword in line for keyword in important_keywords):
                    key_sentences.append(line)
            
            # é™åˆ¶å¥å­æ•°é‡
            key_sentences = key_sentences[:3]
            
            if key_sentences:
                return " | ".join([sentence[:50] + "..." if len(sentence) > 50 else sentence for sentence in key_sentences])
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œè¿”å›å‰å‡ å¥
                non_empty_lines = [line.strip() for line in lines if line.strip()]
                if non_empty_lines:
                    first_sentences = non_empty_lines[:2]
                    return " | ".join([sentence[:50] + "..." if len(sentence) > 50 else sentence for sentence in first_sentences])
                else:
                    return "æ–‡æ¡£å†…å®¹å·²è§£æï¼ŒåŒ…å«ä¸°å¯Œä¿¡æ¯"
                    
        except Exception as e:
            safe_print(f"âš ï¸ å†…å®¹åˆ†æå¤±è´¥: {e}")
            return "æ–‡æ¡£å†…å®¹åˆ†æå®Œæˆ"
    
    def _analyze_table_with_ai(self, result: FileAnalysisResult, user_question: str) -> str:
        """ä½¿ç”¨AIåˆ†æè¡¨æ ¼å†…å®¹"""
        insights = []
        metadata = result.metadata
        
        # æ•°æ®è§„æ¨¡åˆ†æ
        rows = metadata.get('rows', 0)
        cols = metadata.get('columns', 0)
        insights.append(f"ğŸ“Š **æ•°æ®è§„æ¨¡**: {rows} è¡Œ Ã— {cols} åˆ—")
        
        # æ•°æ®ç±»å‹åˆ†æ
        data_types = metadata.get('data_types', {})
        type_counts = {}
        for dtype in data_types.values():
            type_name = str(dtype)
            type_counts[type_name] = type_counts.get(type_name, 0) + 1
        
        if type_counts:
            insights.append(f"ğŸ”¢ **æ•°æ®ç±»å‹**: {', '.join([f'{dtype}({count})' for dtype, count in type_counts.items()])}")
        
        # æ•°æ®è´¨é‡åˆ†æ
        missing_values = sum(1 for col, count in metadata.get('missing_values', {}).items() if count > 0)
        if missing_values == 0:
            insights.append("âœ… **æ•°æ®è´¨é‡**: æ— ç¼ºå¤±å€¼ï¼Œæ•°æ®å®Œæ•´")
        else:
            insights.append(f"âš ï¸ **æ•°æ®è´¨é‡**: {missing_values} åˆ—å­˜åœ¨ç¼ºå¤±å€¼")
        
        # ä¸šåŠ¡ä»·å€¼åˆ†æ
        column_names = metadata.get('column_names', [])
        if any('id' in col.lower() for col in column_names):
            insights.append("ğŸ†” **ä¸šåŠ¡ç‰¹å¾**: åŒ…å«æ ‡è¯†ç¬¦å­—æ®µ")
        if any('date' in col.lower() or 'time' in col.lower() for col in column_names):
            insights.append("â° **æ—¶é—´ç‰¹å¾**: åŒ…å«æ—¶é—´ç›¸å…³å­—æ®µ")
        if any('price' in col.lower() or 'cost' in col.lower() or 'amount' in col.lower() for col in column_names):
            insights.append("ğŸ’° **è´¢åŠ¡ç‰¹å¾**: åŒ…å«é‡‘é¢ç›¸å…³å­—æ®µ")
        
        return "\n".join(insights) if insights else "ğŸ“Š æ ‡å‡†è¡¨æ ¼æ•°æ®"
    
    def _analyze_general_with_ai(self, result: FileAnalysisResult, user_question: str) -> str:
        """ä½¿ç”¨AIåˆ†æä¸€èˆ¬æ–‡ä»¶å†…å®¹"""
        return "ğŸ“„ æ–‡ä»¶å†…å®¹åˆ†æå®Œæˆ"
    
    def _extract_key_concepts(self, content: str) -> list:
        """æå–å…³é”®æ¦‚å¿µ"""
        # ç®€å•çš„å…³é”®è¯æå–
        import re
        
        # æå–å¯èƒ½çš„ä¸“ä¸šæœ¯è¯­ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„è¯ï¼‰
        concepts = re.findall(r'\b[A-Z][a-z]+\b', content)
        
        # æå–å¯èƒ½çš„ç¼©å†™è¯
        abbreviations = re.findall(r'\b[A-Z]{2,}\b', content)
        
        # åˆå¹¶å¹¶å»é‡
        all_concepts = list(set(concepts + abbreviations))
        
        # è¿‡æ»¤æ‰å¸¸è§çš„éä¸“ä¸šè¯æ±‡
        common_words = {'The', 'This', 'That', 'With', 'From', 'They', 'There', 'These', 'Those'}
        filtered_concepts = [concept for concept in all_concepts if concept not in common_words]
        
        return filtered_concepts[:10]  # è¿”å›å‰10ä¸ª
    
    def _extract_key_points(self, result: FileAnalysisResult) -> str:
        """æå–å…³é”®ä¿¡æ¯ç‚¹"""
        key_points = []
        
        if result.file_type == "PDF":
            # PDFå…³é”®ä¿¡æ¯æå–
            content = result.content
            
            # æå–æ•°å­—ä¿¡æ¯
            import re
            numbers = re.findall(r'\d+', content)
            if numbers:
                key_points.append(f"ğŸ“Š åŒ…å« {len(numbers)} ä¸ªæ•°å­—æ•°æ®")
            
            # æå–ç« èŠ‚ä¿¡æ¯
            chapters = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [ï¼š:]\s*([^\n]+)', content)
            if chapters:
                key_points.append(f"ğŸ“š ä¸»è¦ç« èŠ‚: {', '.join(chapters[:3])}")
            
            # æå–æ—¶é—´ä¿¡æ¯
            years = re.findall(r'\b(19|20)\d{2}\b', content)
            if years:
                key_points.append(f"â° æ¶‰åŠå¹´ä»½: {', '.join(sorted(set(years))[:5])}")
        
        elif result.file_type == "TABLE":
            # è¡¨æ ¼å…³é”®ä¿¡æ¯æå–
            metadata = result.metadata
            rows = metadata.get('rows', 0)
            cols = metadata.get('columns', 0)
            
            key_points.append(f"ğŸ“Š æ•°æ®è§„æ¨¡: {rows} è¡Œ Ã— {cols} åˆ—")
            
            column_names = metadata.get('column_names', [])
            if column_names:
                key_points.append(f"ğŸ“ ä¸»è¦å­—æ®µ: {', '.join(column_names[:5])}")
        
        return "\n".join(key_points) if key_points else "ğŸ“„ æ–‡ä»¶å†…å®¹å·²æˆåŠŸè§£æ"

# æµ‹è¯•å‡½æ•°
def test_file_analysis():
    """æµ‹è¯•æ–‡ä»¶åˆ†æåŠŸèƒ½"""
    tool = FileAnalysisTool()
    
    print("ğŸ§ª æµ‹è¯•æ–‡ä»¶åˆ†æå·¥å…·...")
    print(f"æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {tool.get_supported_types()}")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•æ–‡ä»¶è·¯å¾„
    # result = tool.analyze_file("test.pdf")
    # print(tool.generate_ai_analysis(result))

if __name__ == "__main__":
    test_file_analysis()
