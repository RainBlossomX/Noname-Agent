# -*- coding: utf-8 -*-
"""
è®°å¿†ç³»ç»Ÿæ¨¡å— - è¯†åº•æ·±æ¹–
å¤„ç†å¯¹è¯è®°å¿†ã€ä¸»é¢˜æ€»ç»“å’Œä¸Šä¸‹æ–‡å›å¿†
"""

import json
import os
import datetime
import re
import openai
import numpy as np
from config.config import load_config
from src.memory.memory_summary_agent import MemorySummaryAgent
from src.tools.simple_vector_encoder import get_vector_encoder

class MemoryLake:
    """è®°å¿†ç³»ç»Ÿ - è¯†åº•æ·±æ¹–"""
    
    def __init__(self, memory_file="memory_lake.json", chat_logs_dir="chat_logs"):
        self.memory_file = memory_file
        self.chat_logs_dir = chat_logs_dir
        
        # æ–°çš„æ–‡ä»¶ç»“æ„
        self.memory_index_file = os.path.join(chat_logs_dir, "memory_index.json")
        self.memories_dir = os.path.join(chat_logs_dir, "memories")
        self.vectors_dir = os.path.join(chat_logs_dir, "vectors")
        self.daily_logs_dir = os.path.join(chat_logs_dir, "daily_logs")
        
        # åˆå§‹åŒ–ç›®å½•ç»“æ„
        self._init_directory_structure()
        
        # è¿ç§»æ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._migrate_old_data()
        
        self.memory_index = self.load_memory()
        self.current_conversation = []
        self.last_save_date = None
        self.config = load_config()
        
        # è¿ç§»ç›¸å…³æ ‡è®°
        self.pending_migration = None
        
        # åˆå§‹åŒ–è®°å¿†æ€»ç»“AIä»£ç†
        self.summary_agent = MemorySummaryAgent(self.config)
        
        # åˆå§‹åŒ–å‘é‡ç¼–ç å™¨ï¼ˆè¯†åº•æ·±æ¹–å‘é‡æ•°æ®åº“ï¼‰
        try:
            print("ğŸ—„ï¸ è¯†åº•æ·±æ¹–å‘é‡æ•°æ®åº“å·²å¯ç”¨")
        except UnicodeEncodeError:
            print("[INFO] è¯†åº•æ·±æ¹–å‘é‡æ•°æ®åº“å·²å¯ç”¨")
        self.vector_encoder = get_vector_encoder()
        
        # ä¸ºå‘é‡ç¼–ç å™¨æ›´æ–°è¯æ±‡è¡¨ï¼ˆä½¿ç”¨ç°æœ‰è®°å¿†çš„ä¸»é¢˜ï¼‰
        self._update_vector_vocab()
        
        # ğŸš€ ä¿®å¤ï¼šåˆå§‹åŒ–mark_saved_callbackå±æ€§
        self.mark_saved_callback = None
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(self.chat_logs_dir):
            os.makedirs(self.chat_logs_dir)

        # ç¡®ä¿ç¬¬ä¸€æ¡è®°å¿†æ˜¯é‡ç‚¹è®°å¿†
        self.ensure_first_memory_important()
        
        # ä¸ºç°æœ‰è®°å¿†ç”Ÿæˆå‘é‡ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        self._generate_missing_vectors()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªè¿ç§»çš„è®°å¿†æ–‡ä»¶
        self._check_unmigrated_files()

    def _init_directory_structure(self):
        """åˆå§‹åŒ–æ–°çš„ç›®å½•ç»“æ„"""
        directories = [
            self.chat_logs_dir,
            self.memories_dir,
            self.vectors_dir,
            self.daily_logs_dir
        ]
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory)
                print(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")

    def _migrate_old_data(self):
        """æ£€æµ‹æ—§æ•°æ®ï¼Œä¸è‡ªåŠ¨è¿ç§»"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨åªåšæ£€æµ‹ï¼Œä¸æ‰§è¡Œè¿ç§»
        pass

    def _migrate_vector_files(self):
        """è¿ç§»å‘é‡ç›¸å…³æ–‡ä»¶"""
        try:
            # è¿ç§»è¯æ±‡è¡¨æ–‡ä»¶
            old_vocab_file = "topic_vocab.json"
            new_vocab_file = os.path.join(self.vectors_dir, "topic_vocab.json")
            
            if os.path.exists(old_vocab_file) and not os.path.exists(new_vocab_file):
                os.rename(old_vocab_file, new_vocab_file)
                print(f"ğŸ“¦ è¿ç§»è¯æ±‡è¡¨: {old_vocab_file} -> {new_vocab_file}")
            
            # è¿ç§»åµŒå…¥ç¼“å­˜æ–‡ä»¶
            old_embedding_file = "embedding_cache.json"
            new_embedding_file = os.path.join(self.vectors_dir, "embedding_cache.json")
            
            if os.path.exists(old_embedding_file) and not os.path.exists(new_embedding_file):
                os.rename(old_embedding_file, new_embedding_file)
                print(f"ğŸ“¦ è¿ç§»åµŒå…¥ç¼“å­˜: {old_embedding_file} -> {new_embedding_file}")
                
        except Exception as e:
            print(f"âš ï¸ å‘é‡æ–‡ä»¶è¿ç§»å¤±è´¥: {e}")

    def _convert_old_format(self, old_topic):
        """è½¬æ¢è€æ ¼å¼è®°å¿†åˆ°æ–°æ ¼å¼"""
        # åˆ›å»ºæ–°æ ¼å¼çš„è®°å¿†å¯¹è±¡
        new_topic = {
            "topic": old_topic.get("topic", "æœªçŸ¥ä¸»é¢˜"),
            "timestamp": old_topic.get("timestamp", "00:00:00"),
            "date": old_topic.get("date", "unknown"),
            "conversation_count": 1,  # è€æ ¼å¼é»˜è®¤ä¸º1è½®å¯¹è¯
            "keywords": old_topic.get("keywords", []),
            "conversation_details": old_topic.get("context", old_topic.get("conversation_details", "")),
            "is_important": old_topic.get("is_important", False)
        }
        
        # ä¿ç•™å…¶ä»–å¯èƒ½çš„å­—æ®µ
        for key, value in old_topic.items():
            if key not in new_topic:
                new_topic[key] = value
        
        return new_topic
    
    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦"""
        import re
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # é™åˆ¶é•¿åº¦
        if len(sanitized) > 50:
            sanitized = sanitized[:50]
        return sanitized

    def _check_unmigrated_files(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰æœªè¿ç§»çš„è®°å¿†æ–‡ä»¶"""
        try:
            if os.path.exists(self.memory_file):
                print(f"ğŸ” å‘ç°è®°å¿†æ–‡ä»¶: {self.memory_file}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹æ£€æŸ¥
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                
                # è·å–è®°å¿†æ•°é‡
                if isinstance(old_data, dict) and "topics" in old_data:
                    old_memory_count = len(old_data["topics"])
                elif isinstance(old_data, list):
                    old_memory_count = len(old_data)
                else:
                    old_memory_count = 0
                
                # è·å–å½“å‰è®°å¿†æ•°é‡
                current_memory_count = len(self.memory_index.get("topics", []))
                
                print(f"ğŸ“Š æ—§æ–‡ä»¶è®°å¿†æ•°: {old_memory_count}, å½“å‰è®°å¿†æ•°: {current_memory_count}")
                
                if old_memory_count > 0:
                    print("ğŸ’¡ æ£€æµ‹åˆ°æœªè¿ç§»çš„è®°å¿†æ•°æ®ï¼Œå°†é€šè¿‡å¯¹è¯è¯¢é—®ç”¨æˆ·...")
                    # è®¾ç½®è¿ç§»æ ‡è®°ï¼Œè®©AIä¸»åŠ¨è¯¢é—®
                    self.pending_migration = {
                        "old_memory_count": old_memory_count,
                        "current_memory_count": current_memory_count,
                        "old_data": old_data
                    }
                else:
                    print("â„¹ï¸ æ—§æ–‡ä»¶ä¸ºç©ºï¼Œåˆ é™¤...")
                    os.remove(self.memory_file)
                    
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥æœªè¿ç§»æ–‡ä»¶å¤±è´¥: {e}")
            self.pending_migration = None

    def _force_migrate_old_data(self):
        """å¼ºåˆ¶è¿ç§»æ—§æ•°æ®ï¼ˆæ— è®ºæ—¶é—´æˆ³å¦‚ä½•ï¼‰"""
        try:
            print("ğŸ”„ å¼ºåˆ¶è¿ç§»æ¨¡å¼å¯åŠ¨...")
            
            # è¯»å–æ—§æ•°æ®
            with open(self.memory_file, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
            
            # å¤„ç†æ—§æ•°æ®æ ¼å¼
            if isinstance(old_data, dict) and "topics" in old_data:
                new_topics = old_data["topics"]
            elif isinstance(old_data, list):
                new_topics = old_data
            else:
                print("âš ï¸ æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
                return
            
            if not new_topics:
                print("âš ï¸ æ—§æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è®°å¿†æ•°æ®")
                return
            
            # è¯»å–ç°æœ‰ç´¢å¼•
            existing_memory_index = {"memories": []}
            if os.path.exists(self.memory_index_file):
                try:
                    with open(self.memory_index_file, 'r', encoding='utf-8') as f:
                        existing_memory_index = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–ç°æœ‰ç´¢å¼•å¤±è´¥: {e}")
            
            # è·å–ç°æœ‰è®°å¿†IDé›†åˆ
            existing_ids = set()
            for memory_info in existing_memory_index.get("memories", []):
                existing_ids.add(memory_info.get("id", ""))
            
            # è¿ç§»æ–°è®°å¿†
            new_count = 0
            duplicate_count = 0
            
            for i, topic in enumerate(new_topics):
                date = topic.get("date", "unknown")
                timestamp = topic.get("timestamp", "00-00-00")
                memory_id = f"{date}_{timestamp.replace(':', '-')}"
                
                # æ£€æŸ¥é‡å¤
                if memory_id in existing_ids:
                    duplicate_count += 1
                    continue
                
                # è½¬æ¢æ ¼å¼å¹¶ä¿å­˜
                converted_topic = self._convert_old_format(topic)
                
                timestamp_clean = timestamp.replace(":", "-")
                topic_name = self._sanitize_filename(topic.get("topic", f"imported_{i}"))
                memory_filename = f"{date}_{timestamp_clean}_{topic_name}.json"
                memory_filepath = os.path.join(self.memories_dir, memory_filename)
                
                with open(memory_filepath, 'w', encoding='utf-8') as f:
                    json.dump(converted_topic, f, ensure_ascii=False, indent=2)
                
                # æ›´æ–°ç´¢å¼•
                existing_memory_index["memories"].append({
                    "id": memory_id,
                    "filename": memory_filename,
                    "topic": topic.get("topic", ""),
                    "date": date,
                    "timestamp": timestamp,
                    "is_important": topic.get("is_important", False)
                })
                
                new_count += 1
            
            # ä¿å­˜ç´¢å¼•
            with open(self.memory_index_file, 'w', encoding='utf-8') as f:
                json.dump(existing_memory_index, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¼ºåˆ¶è¿ç§»å®Œæˆ: æ–°å¢ {new_count} æ¡è®°å¿†ï¼Œè·³è¿‡ {duplicate_count} æ¡é‡å¤")
            
            # å¤‡ä»½åŸæ–‡ä»¶
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"{self.memory_file}.migrated_{timestamp}"
            os.rename(self.memory_file, backup_file)
            print(f"ğŸ“¦ å·²å¤‡ä»½å¹¶ç§»é™¤æ—§æ–‡ä»¶: {backup_file}")
            
        except Exception as e:
            print(f"âš ï¸ å¼ºåˆ¶è¿ç§»å¤±è´¥: {e}")

    def get_migration_status(self):
        """è·å–è¿ç§»çŠ¶æ€"""
        return self.pending_migration
    
    def confirm_migration(self, user_response):
        """ç”¨æˆ·ç¡®è®¤è¿ç§»"""
        if not self.pending_migration:
            return "æ²¡æœ‰å¾…è¿ç§»çš„æ•°æ®ã€‚"
        
        if user_response.strip().lower() in ['æ˜¯', 'yes', 'y', 'ç¡®è®¤', 'åŒæ„']:
            try:
                print("âœ… ç”¨æˆ·ç¡®è®¤è¿ç§»ï¼Œå¼€å§‹æ‰§è¡Œ...")
                old_data = self.pending_migration["old_data"]
                
                # æ‰§è¡Œè¿ç§»
                success = self._execute_migration(old_data)
                
                if success:
                    self.pending_migration = None
                    return "âœ… è®°å¿†è¿ç§»å®Œæˆï¼æ‚¨çš„æ‰€æœ‰å†å²å¯¹è¯è®°å½•å·²æˆåŠŸè½¬ç§»åˆ°æ–°çš„æ–‡ä»¶ç»“æ„ä¸­ï¼Œç°åœ¨å¯ä»¥äº«å—æ›´æ™ºèƒ½çš„å›å¿†åŠŸèƒ½äº†ã€‚"
                else:
                    return "âŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿæ—¥å¿—ã€‚åŸå§‹æ•°æ®å·²å®‰å…¨ä¿ç•™ã€‚"
                    
            except Exception as e:
                print(f"âš ï¸ è¿ç§»æ‰§è¡Œå¤±è´¥: {e}")
                return f"âŒ è¿ç§»å¤±è´¥ï¼š{str(e)}"
                
        elif user_response.strip().lower() in ['å¦', 'no', 'n', 'å–æ¶ˆ', 'æ‹’ç»']:
            print("âŒ ç”¨æˆ·å–æ¶ˆè¿ç§»")
            self.pending_migration = None
            return "å¥½çš„ï¼Œå·²å–æ¶ˆè¿ç§»ã€‚æ‚¨çš„æ—§è®°å¿†æ–‡ä»¶å°†ä¿æŒåŸæ ·ï¼Œä½†æ— æ³•ä½¿ç”¨æ–°çš„æ™ºèƒ½å›å¿†åŠŸèƒ½ã€‚å¦‚éœ€è¿ç§»ï¼Œè¯·é‡æ–°å¯åŠ¨éœ²å°¼è¥¿äºšã€‚"
        else:
            return "è¯·å›ç­”'æ˜¯'æˆ–'å¦'æ¥ç¡®è®¤æ˜¯å¦è¿›è¡Œè®°å¿†è¿ç§»ã€‚"
    
    def _execute_migration(self, old_data):
        """æ‰§è¡Œè®°å¿†è¿ç§»"""
        try:
            # å¤„ç†æ—§æ•°æ®æ ¼å¼
            if isinstance(old_data, dict) and "topics" in old_data:
                new_topics = old_data["topics"]
            elif isinstance(old_data, list):
                new_topics = old_data
            else:
                print("âš ï¸ æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
                return False
            
            if not new_topics:
                print("âš ï¸ æ—§æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è®°å¿†æ•°æ®")
                return False
            
            # è¯»å–ç°æœ‰ç´¢å¼•
            existing_memory_index = {"memories": []}
            if os.path.exists(self.memory_index_file):
                try:
                    with open(self.memory_index_file, 'r', encoding='utf-8') as f:
                        existing_memory_index = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–ç°æœ‰ç´¢å¼•å¤±è´¥: {e}")
            
            # è·å–ç°æœ‰è®°å¿†IDé›†åˆ
            existing_ids = set()
            for memory_info in existing_memory_index.get("memories", []):
                existing_ids.add(memory_info.get("id", ""))
            
            # è¿ç§»æ–°è®°å¿†
            new_count = 0
            duplicate_count = 0
            
            for i, topic in enumerate(new_topics):
                date = topic.get("date", "unknown")
                timestamp = topic.get("timestamp", "00-00-00")
                memory_id = f"{date}_{timestamp.replace(':', '-')}"
                
                # æ£€æŸ¥é‡å¤
                if memory_id in existing_ids:
                    duplicate_count += 1
                    continue
                
                # è½¬æ¢æ ¼å¼å¹¶ä¿å­˜
                converted_topic = self._convert_old_format(topic)
                
                timestamp_clean = timestamp.replace(":", "-")
                topic_name = self._sanitize_filename(topic.get("topic", f"imported_{i}"))
                memory_filename = f"{date}_{timestamp_clean}_{topic_name}.json"
                memory_filepath = os.path.join(self.memories_dir, memory_filename)
                
                with open(memory_filepath, 'w', encoding='utf-8') as f:
                    json.dump(converted_topic, f, ensure_ascii=False, indent=2)
                
                # æ›´æ–°ç´¢å¼•
                existing_memory_index["memories"].append({
                    "id": memory_id,
                    "filename": memory_filename,
                    "topic": topic.get("topic", ""),
                    "date": date,
                    "timestamp": timestamp,
                    "is_important": topic.get("is_important", False)
                })
                
                new_count += 1
            
            # ä¿å­˜ç´¢å¼•
            with open(self.memory_index_file, 'w', encoding='utf-8') as f:
                json.dump(existing_memory_index, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… è¿ç§»å®Œæˆ: æ–°å¢ {new_count} æ¡è®°å¿†ï¼Œè·³è¿‡ {duplicate_count} æ¡é‡å¤")
            
            # å¤‡ä»½å¹¶åˆ é™¤åŸæ–‡ä»¶
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"{self.memory_file}.migrated_{timestamp}"
            os.rename(self.memory_file, backup_file)
            print(f"ğŸ“¦ å·²å¤‡ä»½å¹¶ç§»é™¤æ—§æ–‡ä»¶: {backup_file}")
            
            # é‡æ–°åŠ è½½è®°å¿†
            self.memory_index = self.load_memory()
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ è¿ç§»æ‰§è¡Œå¤±è´¥: {e}")
            return False

    def load_memory(self):
        """åŠ è½½è®°å¿†ç´¢å¼•å’Œè®°å¿†æ•°æ®"""
        try:
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„ç´¢å¼•æ–‡ä»¶
            if os.path.exists(self.memory_index_file):
                with open(self.memory_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                # åŠ è½½æ‰€æœ‰è®°å¿†æ•°æ®
                topics = []
                for memory_info in index_data.get("memories", []):
                    memory_filepath = os.path.join(self.memories_dir, memory_info["filename"])
                    if os.path.exists(memory_filepath):
                        try:
                            with open(memory_filepath, 'r', encoding='utf-8') as f:
                                memory_data = json.load(f)
                                topics.append(memory_data)
                        except Exception as e:
                            print(f"âš ï¸ åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥: {memory_info['filename']}, {e}")
                
                return {"topics": topics, "conversations": {}, "contexts": {}}
            
            # å…¼å®¹æ—§æ ¼å¼
            elif os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return {"topics": data, "conversations": {}, "contexts": {}}
                    elif isinstance(data, dict):
                        return data
                    else:
                        return {"topics": [], "conversations": {}, "contexts": {}}
            
            return {"topics": [], "conversations": {}, "contexts": {}}
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®°å¿†å¤±è´¥: {e}")
            return {"topics": [], "conversations": {}, "contexts": {}}

    def save_memory(self):
        """ä¿å­˜è®°å¿†ï¼ˆæ™ºèƒ½é€‰æ‹©ä¿å­˜æ–¹å¼ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨æ–°çš„æ–‡ä»¶ç»“æ„
        if os.path.exists(self.memory_index_file) or os.path.exists(self.memories_dir):
            # ä½¿ç”¨æ–°çš„æ–‡ä»¶ç»“æ„
            self._save_to_new_structure()
        else:
            # ä½¿ç”¨æ—§çš„æ–‡ä»¶ç»“æ„ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
            self._save_to_old_structure()

    def _save_to_new_structure(self):
        """ä¿å­˜åˆ°æ–°çš„æ–‡ä»¶ç»“æ„"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not os.path.exists(self.memories_dir):
                os.makedirs(self.memories_dir)
            
            # ä¿å­˜ç´¢å¼•æ–‡ä»¶
            memory_index = {"memories": []}
            
            for topic in self.memory_index.get("topics", []):
                # ç”Ÿæˆæ–‡ä»¶å
                date = topic.get("date", "unknown")
                timestamp = topic.get("timestamp", "00-00-00").replace(":", "-")
                topic_name = self._sanitize_filename(topic.get("topic", "unknown"))
                
                memory_filename = f"{date}_{timestamp}_{topic_name}.json"
                memory_filepath = os.path.join(self.memories_dir, memory_filename)
                
                # ä¿å­˜å•ç‹¬çš„è®°å¿†æ–‡ä»¶
                with open(memory_filepath, 'w', encoding='utf-8') as f:
                    json.dump(topic, f, ensure_ascii=False, indent=2)
                
                # åœ¨ç´¢å¼•ä¸­è®°å½•
                memory_index["memories"].append({
                    "id": f"{date}_{timestamp}",
                    "filename": memory_filename,
                    "topic": topic.get("topic", ""),
                    "date": date,
                    "timestamp": topic.get("timestamp", ""),
                    "is_important": topic.get("is_important", False)
                })
            
            # ä¿å­˜ç´¢å¼•æ–‡ä»¶
            with open(self.memory_index_file, 'w', encoding='utf-8') as f:
                json.dump(memory_index, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æ–°ç»“æ„ä¿å­˜: {len(self.memory_index.get('topics', []))} æ¡è®°å¿†")
            
        except Exception as e:
            print(f"âš ï¸ æ–°ç»“æ„ä¿å­˜å¤±è´¥: {e}")
            # é™çº§åˆ°æ—§ç»“æ„
            self._save_to_old_structure()

    def _save_to_old_structure(self):
        """ä¿å­˜åˆ°æ—§çš„æ–‡ä»¶ç»“æ„ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰"""
        try:
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.memory_index, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ å…¼å®¹æ¨¡å¼ä¿å­˜: {len(self.memory_index.get('topics', []))} æ¡è®°å¿†")
        except Exception as e:
            print(f"âŒ å…¼å®¹æ¨¡å¼ä¿å­˜å¤±è´¥: {e}")

    def _update_vector_vocab(self):
        """æ›´æ–°å‘é‡ç¼–ç å™¨çš„è¯æ±‡è¡¨"""
        try:
            # æ”¶é›†æ‰€æœ‰ä¸»é¢˜æ–‡æœ¬
            topics = []
            for entry in self.memory_index.get("topics", []):
                if "topic" in entry:
                    topics.append(entry["topic"])
            
            if topics:
                print(f"ğŸ“š æ›´æ–°å‘é‡è¯æ±‡è¡¨ï¼Œä½¿ç”¨ {len(topics)} ä¸ªä¸»é¢˜")
                self.vector_encoder.update_vocab(topics)
            else:
                print("ğŸ“š æ— ç°æœ‰ä¸»é¢˜ï¼Œä½¿ç”¨ç©ºè¯æ±‡è¡¨")
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°å‘é‡è¯æ±‡è¡¨å¤±è´¥: {e}")
    
    def _generate_missing_vectors(self):
        """ä¸ºç¼ºå¤±å‘é‡çš„è®°å¿†ç”ŸæˆåŒé‡å‘é‡"""
        try:
            updated_count = 0
            for entry in self.memory_index.get("topics", []):
                needs_update = False
                
                # ç”Ÿæˆä¸»é¢˜å‘é‡ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
                if "topic_vector" not in entry and "topic" in entry:
                    topic_vector = self.vector_encoder.encode_text(entry["topic"])
                    if topic_vector:
                        entry["topic_vector"] = topic_vector
                        needs_update = True
                
                # ç”Ÿæˆå†…å®¹å‘é‡ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
                if "details_vector" not in entry and "conversation_details" in entry:
                    details_vector = self.vector_encoder.encode_text(entry["conversation_details"])
                    if details_vector:
                        entry["details_vector"] = details_vector
                        needs_update = True
                
                if needs_update:
                    updated_count += 1
            
            if updated_count > 0:
                print(f"ğŸ”„ ä¸º {updated_count} æ¡è®°å¿†ç”Ÿæˆäº†åŒé‡å‘é‡")
                self.save_memory()
            else:
                print("âœ… æ‰€æœ‰è®°å¿†éƒ½å·²æœ‰åŒé‡å‘é‡")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç¼ºå¤±å‘é‡å¤±è´¥: {e}")
    
    def get_vector_stats(self) -> dict:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        encoder_stats = self.vector_encoder.get_stats()
        
        # ç»Ÿè®¡å‘é‡è¦†ç›–æƒ…å†µ
        total_count = len(self.memory_index.get("topics", []))
        topic_vector_count = 0
        details_vector_count = 0
        dual_vector_count = 0
        
        for entry in self.memory_index.get("topics", []):
            has_topic = "topic_vector" in entry and entry["topic_vector"]
            has_details = "details_vector" in entry and entry["details_vector"]
            
            if has_topic:
                topic_vector_count += 1
            if has_details:
                details_vector_count += 1
            if has_topic and has_details:
                dual_vector_count += 1
        
        return {
            "encoder_stats": encoder_stats,
            "total_memories": total_count,
            "topic_vectorized": topic_vector_count,
            "details_vectorized": details_vector_count,
            "dual_vectorized": dual_vector_count,
            "dual_vectorization_rate": f"{dual_vector_count/total_count*100:.1f}%" if total_count > 0 else "0%"
        }
    
    def _get_timestamp_score(self, memory_entry):
        """è®¡ç®—æ—¶é—´æˆ³åˆ†æ•°ï¼Œç”¨äºæ’åº"""
        try:
            date_str = memory_entry.get("date", "")
            timestamp_str = memory_entry.get("timestamp", "")
            
            if date_str and timestamp_str:
                # ç»„åˆæ—¥æœŸå’Œæ—¶é—´
                datetime_str = f"{date_str} {timestamp_str}"
                memory_datetime = datetime.datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
                
                # è½¬æ¢ä¸ºæ—¶é—´æˆ³åˆ†æ•°ï¼ˆè¶Šæ–°åˆ†æ•°è¶Šé«˜ï¼‰
                timestamp = memory_datetime.timestamp()
                return timestamp
            else:
                return 0
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æ—¶é—´æˆ³åˆ†æ•°å¤±è´¥: {e}")
            return 0

    def add_conversation(self, user_input, ai_response, developer_mode=False, mark_saved_callback=None):
        """æ·»åŠ å¯¹è¯åˆ°å½“å‰ä¼šè¯"""
        # å¼€å‘è€…æ¨¡å¼ä¸‹ä¸ä¿å­˜åˆ°è®°å¿†ç³»ç»Ÿ
        if developer_mode:
            print("ğŸ”§ å¼€å‘è€…æ¨¡å¼å·²å¼€å¯ï¼Œè·³è¿‡å¯¹è¯è®°å½•åˆ°è®°å¿†ç³»ç»Ÿ")
            return
        
        # ğŸš€ ä¿®å¤ï¼šé˜²é‡å¤æ·»åŠ æœºåˆ¶ï¼ˆæ£€æŸ¥æœ€è¿‘çš„å¯¹è¯ï¼‰
        # æ£€æŸ¥æœ€è¿‘çš„å¯¹è¯æ˜¯å¦æ˜¯ç›¸åŒçš„ç”¨æˆ·è¾“å…¥ï¼ˆæ—¶é—´çª—å£å†…ï¼‰
        if self.current_conversation:
            last_conv = self.current_conversation[-1]
            # å¦‚æœä¸Šä¸€æ¡å¯¹è¯çš„ç”¨æˆ·è¾“å…¥ç›¸åŒï¼Œä¸”æ—¶é—´é—´éš”å°äº5ç§’ï¼Œè®¤ä¸ºæ˜¯é‡å¤
            if last_conv.get('user_input') == user_input:
                try:
                    last_time = datetime.datetime.strptime(last_conv['timestamp'], "%H:%M:%S")
                    current_time = datetime.datetime.now()
                    # æ„é€ ä»Šå¤©çš„å®Œæ•´æ—¶é—´
                    last_datetime = datetime.datetime.combine(
                        datetime.datetime.now().date(),
                        last_time.time()
                    )
                    time_diff = (current_time - last_datetime).total_seconds()
                    
                    if time_diff < 5:  # 5ç§’å†…çš„é‡å¤è®¤ä¸ºæ˜¯åŒä¸€æ¬¡å¯¹è¯
                        print(f"âš ï¸ æ£€æµ‹åˆ°5ç§’å†…é‡å¤ç”¨æˆ·è¾“å…¥ï¼Œè·³è¿‡æ·»åŠ : {user_input[:30]}...")
                        return
                except Exception as e:
                    # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæ¥çš„ç®€å•æ£€æŸ¥
                    print(f"âš ï¸ æ—¶é—´è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•é‡å¤æ£€æŸ¥")
                    if last_conv.get('ai_response') == ai_response:
                        print(f"âš ï¸ æ£€æµ‹åˆ°å®Œå…¨é‡å¤å¯¹è¯ï¼Œè·³è¿‡æ·»åŠ : {user_input[:30]}...")
                        return
        
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        self.current_conversation.append({
            "timestamp": timestamp,
            "user_input": user_input,
            "ai_response": ai_response,
            "full_text": f"æŒ‡æŒ¥å®˜: {user_input}\néœ²å°¼è¥¿äºš: {ai_response}"
        })
        
        print(f"âœ… æ·»åŠ å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ: {user_input[:30]}... (å½“å‰å…±{len(self.current_conversation)}æ¡)")
        
        # ğŸš€ ä¿®å¤ï¼šä¿å­˜å›è°ƒå‡½æ•°ï¼Œåœ¨å¯¹è¯çœŸæ­£ä¿å­˜åˆ°è¯†åº•æ·±æ¹–åè°ƒç”¨
        if mark_saved_callback:
            self.mark_saved_callback = mark_saved_callback

    def should_summarize(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ€»ç»“"""
        # æ¯3æ¡å¯¹è¯æ€»ç»“ä¸€æ¬¡ï¼Œæˆ–è€…å½“å‰å¯¹è¯è¶…è¿‡5æ¡
        return len(self.current_conversation) >= 3

    def summarize_and_save_topic(self, ai_client=None, force_save=False):
        """æ€»ç»“å¹¶ä¿å­˜ä¸»é¢˜"""
        if not self.current_conversation:
            return None
        
        # å¦‚æœä¸æ˜¯å¼ºåˆ¶ä¿å­˜ï¼Œæ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¿å­˜æ¡ä»¶
        if not force_save and not self.should_summarize():
            return None
            
        try:
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = "\n".join([
                conv["full_text"] for conv in self.current_conversation
            ])
            
            # ä½¿ç”¨AIæ€»ç»“ä¸»é¢˜
            topic = self._ai_summarize_topic(conversation_text)
            
            # ä¿å­˜åˆ°è®°å¿†ç´¢å¼•
            timestamp = datetime.datetime.now().strftime("%H:%M:%S")
            date_str = datetime.datetime.now().strftime("%Y-%m-%d")
            
            # ç”ŸæˆåŒé‡å‘é‡ï¼ˆè¯†åº•æ·±æ¹–å‘é‡æ•°æ®åº“å‡çº§ç‰ˆï¼‰
            conversation_details = self._extract_conversation_details()
            topic_vector = self.vector_encoder.encode_text(topic)
            details_vector = self.vector_encoder.encode_text(conversation_details)
            
            entry = {
                "topic": topic,
                "timestamp": timestamp,
                "date": date_str,
                "conversation_count": len(self.current_conversation),
                "keywords": self._extract_keywords(conversation_text),  # ä¿ç•™å…³é”®è¯ä½œä¸ºå¤‡ç”¨
                "conversation_details": conversation_details,
                "topic_vector": topic_vector if topic_vector is not None else None,
                "details_vector": details_vector if details_vector is not None else None,
                "is_important": False  # é‡ç‚¹è®°å¿†æ ‡ç­¾
            }
            
            self.memory_index["topics"].append(entry)
            self.save_memory()
            
            # æ›´æ–°å‘é‡ç¼–ç å™¨è¯æ±‡è¡¨ï¼ˆåŒ…å«æ–°ä¸»é¢˜ï¼‰
            self.vector_encoder.update_vocab([topic] + [e.get("topic", "") for e in self.memory_index["topics"]])
            
            # ğŸš€ ä¿®å¤ï¼šåœ¨æˆåŠŸä¿å­˜åˆ°è¯†åº•æ·±æ¹–åï¼Œæ ‡è®°æ‰€æœ‰å·²ä¿å­˜çš„å¯¹è¯ä¸ºå·²ä¿å­˜
            # è·å–AIä»£ç†çš„mark_saved_callbackå‡½æ•°
            if hasattr(self, 'mark_saved_callback') and self.mark_saved_callback:
                for conv in self.current_conversation:
                    self.mark_saved_callback(conv['user_input'], conv['ai_response'])
            
            # æ¸…ç©ºå½“å‰ä¼šè¯
            self.current_conversation = []
            
            return topic
            
        except Exception as e:
            print(f"æ€»ç»“ä¸»é¢˜å¤±è´¥: {str(e)}")
            return None

    def force_save_current_conversation(self, introduction_content=None):
        """å¼ºåˆ¶ä¿å­˜å½“å‰å¯¹è¯ï¼ˆç”¨äºé¦–æ¬¡ä»‹ç»åç«‹å³ä¿å­˜ï¼‰"""
        if not self.current_conversation:
            return None
            
        try:
            # æ„å»ºå¯¹è¯æ–‡æœ¬ï¼ŒåŒ…å«è‡ªæˆ‘ä»‹ç»
            conversation_parts = []
            
            # å¦‚æœæœ‰è‡ªæˆ‘ä»‹ç»å†…å®¹ï¼Œæ·»åŠ åˆ°å¼€å¤´
            if introduction_content:
                conversation_parts.append(f"éœ²å°¼è¥¿äºš: {introduction_content}")
                conversation_parts.append("")  # ç©ºè¡Œåˆ†éš”
            
            # æ·»åŠ å®é™…å¯¹è¯å†…å®¹
            for conv in self.current_conversation:
                conversation_parts.append(conv["full_text"])
            
            conversation_text = "\n".join(conversation_parts)
            
            # ä¸ºé¦–æ¬¡å¯¹è¯ç”Ÿæˆç‰¹æ®Šä¸»é¢˜
            topic = self._generate_first_conversation_topic(conversation_text)
            
            # ä¿å­˜åˆ°è®°å¿†ç´¢å¼•
            timestamp = datetime.datetime.now().strftime("%H:%M:%S")
            date_str = datetime.datetime.now().strftime("%Y-%m-%d")
            
            # ç”Ÿæˆå¯¹è¯è¯¦æƒ…
            conversation_details = self._extract_conversation_details()
            
            # ğŸ¯ åªæœ‰åœ¨çœŸæ­£çš„é¦–æ¬¡å¯¹è¯æ—¶æ‰æ·»åŠ è‡ªæˆ‘ä»‹ç»
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¦–æ¬¡å¯¹è¯ï¼šå½“å‰å¯¹è¯æ•°=1 ä¸”æœ‰è‡ªæˆ‘ä»‹ç»å†…å®¹
            if introduction_content and len(self.current_conversation) == 1:
                # ç®€åŒ–è‡ªæˆ‘ä»‹ç»å†…å®¹
                intro_summary = "éœ²å°¼è¥¿äºš: æˆ‘æ˜¯éœ²å°¼è¥¿äºšï¼Œå¨å»‰çš„å§å§ï¼Œæ‚¨çš„AIåŠ©æ‰‹ã€‚å…·å¤‡æ™ºèƒ½å¯¹è¯ã€å¤©æ°”æŸ¥è¯¢ã€éŸ³ä¹æ¨èã€æ–‡ä»¶ç®¡ç†ã€ç¼–ç¨‹ä»£ç ç”Ÿæˆã€å¤šè¯­è¨€äº¤æµåŠè®°å¿†ç³»ç»Ÿ\"è¯†åº•æ·±æ¹–\"ç­‰åŠŸèƒ½ã€‚\n\n"
                conversation_details = intro_summary + conversation_details
                print(f"ğŸ¯ çœŸæ­£çš„é¦–æ¬¡å¯¹è¯ï¼Œå·²æ·»åŠ è‡ªæˆ‘ä»‹ç»ï¼Œæ€»é•¿åº¦: {len(conversation_details)} å­—ç¬¦")
            elif introduction_content:
                print(f"âš ï¸ æ£€æµ‹åˆ°è‡ªæˆ‘ä»‹ç»å†…å®¹ä½†ä¸æ˜¯é¦–æ¬¡å¯¹è¯ï¼ˆå¯¹è¯æ•°: {len(self.current_conversation)}ï¼‰ï¼Œè·³è¿‡æ·»åŠ ")
            
            topic_vector = self.vector_encoder.encode_text(topic)
            details_vector = self.vector_encoder.encode_text(conversation_details)
            
            entry = {
                "topic": topic,
                "timestamp": timestamp,
                "date": date_str,
                "conversation_count": len(self.current_conversation),
                "keywords": self._extract_keywords(conversation_text),
                "conversation_details": conversation_details,
                "topic_vector": topic_vector if topic_vector is not None else None,
                "details_vector": details_vector if details_vector is not None else None,
                "is_important": True,  # é¦–æ¬¡å¯¹è¯æ ‡è®°ä¸ºé‡è¦
                "is_first_conversation": True  # æ ‡è®°ä¸ºé¦–æ¬¡å¯¹è¯
            }
            
            self.memory_index["topics"].append(entry)
            self.save_memory()
            
            # æ›´æ–°å‘é‡ç¼–ç å™¨è¯æ±‡è¡¨
            self.vector_encoder.update_vocab([topic] + [e.get("topic", "") for e in self.memory_index["topics"]])
            
            # æ ‡è®°å¯¹è¯ä¸ºå·²ä¿å­˜
            if hasattr(self, 'mark_saved_callback') and self.mark_saved_callback:
                for conv in self.current_conversation:
                    self.mark_saved_callback(conv['user_input'], conv['ai_response'])
            
            # æ¸…ç©ºå½“å‰ä¼šè¯
            self.current_conversation = []
            
            return topic
            
        except Exception as e:
            print(f"âš ï¸ å¼ºåˆ¶ä¿å­˜é¦–æ¬¡å¯¹è¯å¤±è´¥: {str(e)}")
            return None

    def _generate_first_conversation_topic(self, conversation_text):
        """ä¸ºé¦–æ¬¡å¯¹è¯ç”Ÿæˆç‰¹æ®Šä¸»é¢˜"""
        try:
            # å°è¯•ä½¿ç”¨AIæ€»ç»“
            topic = self._ai_summarize_topic(conversation_text)
            if topic:
                return f"é¦–æ¬¡ç›¸é‡ - {topic}"
        except Exception as e:
            print(f"âš ï¸ AIæ€»ç»“é¦–æ¬¡å¯¹è¯ä¸»é¢˜å¤±è´¥: {e}")
        
        # å¦‚æœAIæ€»ç»“å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆ
        lines = conversation_text.split('\n')
        user_lines = [line for line in lines if line.startswith('æŒ‡æŒ¥å®˜:')]
        if user_lines:
            first_user_input = user_lines[0].replace('æŒ‡æŒ¥å®˜:', '').strip()
            if len(first_user_input) > 10:
                first_user_input = first_user_input[:10] + "..."
            return f"é¦–æ¬¡ç›¸é‡ - {first_user_input}"
        
        return "é¦–æ¬¡ç›¸é‡å¯¹è¯"

    def _format_first_conversation_fallback(self, conversation_text):
        """é¦–æ¬¡å¯¹è¯çš„å¤‡ç”¨æ ¼å¼åŒ–æ–¹æ¡ˆ"""
        try:
            lines = conversation_text.split('\n')
            formatted_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç®€åŒ–æ ¼å¼
                if line.startswith('éœ²å°¼è¥¿äºš:'):
                    content = line[5:].strip()  # ç§»é™¤"éœ²å°¼è¥¿äºš:"
                    if 'æˆ‘æ˜¯éœ²å°¼è¥¿äºš' in content:
                        # ç®€åŒ–è‡ªæˆ‘ä»‹ç»
                        formatted_lines.append("éœ²å°¼è¥¿äºš: æˆ‘æ˜¯éœ²å°¼è¥¿äºšï¼Œå¨å»‰çš„å§å§ã€‚AIåŠ©æ‰‹ï¼Œå…·å¤‡æ™ºèƒ½å¯¹è¯ã€å¤©æ°”æŸ¥è¯¢ã€éŸ³ä¹æ¨èã€æ–‡ä»¶ç®¡ç†ã€ç¼–ç¨‹å¸®åŠ©ã€å¤šè¯­è¨€ç¿»è¯‘å’Œè®°å¿†ç³»ç»Ÿç­‰åŠŸèƒ½ã€‚")
                    elif len(content) > 100:
                        # é•¿å›å¤æˆªå–å‰100å­—ç¬¦
                        formatted_lines.append(f"éœ²å°¼è¥¿äºš: {content[:100]}...")
                    else:
                        formatted_lines.append(line)
                elif line.startswith('æŒ‡æŒ¥å®˜:'):
                    formatted_lines.append(line)
            
            result = '\n'.join(formatted_lines)
            print(f"âœ… é¦–æ¬¡å¯¹è¯å¤‡ç”¨æ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            print(f"âš ï¸ é¦–æ¬¡å¯¹è¯å¤‡ç”¨æ ¼å¼åŒ–å¤±è´¥: {e}")
            return conversation_text[:500] + "..." if len(conversation_text) > 500 else conversation_text

    def _ai_summarize_topic(self, conversation_text):
        """ä½¿ç”¨AIæ€»ç»“ä¸»é¢˜"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ å°è¯•AIä¸»é¢˜æ€»ç»“ (ç¬¬{attempt + 1}æ¬¡)")
                # ä½¿ç”¨ä¸“é—¨çš„è®°å¿†æ€»ç»“AIä»£ç†
                topic = self.summary_agent.summarize_topic(conversation_text)
                if topic and len(topic.strip()) >= 2:
                    print(f"âœ… AIä¸»é¢˜æ€»ç»“æˆåŠŸ: {topic}")
                    return topic
                else:
                    print(f"âš ï¸ AIä¸»é¢˜æ€»ç»“è¿”å›ç©ºç»“æœ (ç¬¬{attempt + 1}æ¬¡)")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print("âŒ AIä¸»é¢˜æ€»ç»“æœ€ç»ˆå¤±è´¥")
                        return "AIæ€»ç»“å¤±è´¥"
            except Exception as e:
                print(f"âš ï¸ AIä¸»é¢˜æ€»ç»“å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {str(e)}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                    import time
                    time.sleep(2)
                    continue
                else:
                    print("âŒ AIä¸»é¢˜æ€»ç»“æœ€ç»ˆå¤±è´¥")
                    return "AIæ€»ç»“å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®"

    def _ai_summarize_content(self, conversation_text):
        """ä½¿ç”¨AIæ€»ç»“å†…å®¹"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ å°è¯•AIä¸Šä¸‹æ–‡æ€»ç»“ (ç¬¬{attempt + 1}æ¬¡)")
                # ä½¿ç”¨ä¸“é—¨çš„è®°å¿†æ€»ç»“AIä»£ç†
                summary = self.summary_agent.summarize_context(conversation_text)
                if summary and len(summary.strip()) > 10:
                    print(f"âœ… AIä¸Šä¸‹æ–‡æ€»ç»“æˆåŠŸ: {summary[:50]}...")
                    return summary
                else:
                    print(f"âš ï¸ AIä¸Šä¸‹æ–‡æ€»ç»“è¿”å›ç©ºç»“æœ (ç¬¬{attempt + 1}æ¬¡)")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print("âŒ AIä¸Šä¸‹æ–‡æ€»ç»“æœ€ç»ˆå¤±è´¥")
                        return "AIæ€»ç»“å¤±è´¥"
            except Exception as e:
                print(f"âš ï¸ AIä¸Šä¸‹æ–‡æ€»ç»“å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {str(e)}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                    import time
                    time.sleep(2)
                    continue
                else:
                    print("âŒ AIä¸Šä¸‹æ–‡æ€»ç»“æœ€ç»ˆå¤±è´¥")
                    return "AIæ€»ç»“å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®"

    def _simple_summarize_topic(self, text):
        """ç®€å•ä¸»é¢˜æ€»ç»“ - åˆ†ææ•´ä¸ªå¯¹è¯æµç¨‹"""
        topics = []
        
        # åˆ†æå„ç§ä¸»é¢˜ç±»å‹
        if "Python" in text or "python" in text:
            topics.append("Pythonç¼–ç¨‹")
        if "C++" in text or "c++" in text:
            topics.append("C++ç¼–ç¨‹")
        if "COBOL" in text or "cobol" in text:
            topics.append("COBOLç¼–ç¨‹")
        if "java" in text or "Java" in text:
            topics.append("Javaç¼–ç¨‹")
        if "éŸ³ä¹" in text or "æ­Œå•" in text or "æ­Œæ›²" in text:
            topics.append("éŸ³ä¹æ¨è")
        if "å¤©æ°”" in text:
            topics.append("å¤©æ°”æŸ¥è¯¢")
        if "æ–‡ä»¶" in text and ("åˆ›å»º" in text or "ä¿å­˜" in text):
            topics.append("æ–‡ä»¶æ“ä½œ")
        if "æ–‡ä»¶å¤¹" in text or "ç›®å½•" in text:
            topics.append("æ–‡ä»¶å¤¹åˆ›å»º")
        if "è®¡ç®—å™¨" in text:
            topics.append("è®¡ç®—å™¨ç¨‹åº")
        if "ä¿„ç½—æ–¯æ–¹å—" in text or "tetris" in text:
            topics.append("ä¿„ç½—æ–¯æ–¹å—æ¸¸æˆ")
        if "è´ªåƒè›‡" in text or "snake" in text:
            topics.append("è´ªåƒè›‡æ¸¸æˆ")
        if "äº•å­—æ£‹" in text or "tic-tac-toe" in text:
            topics.append("äº•å­—æ£‹æ¸¸æˆ")
        if "çˆ¬è™«" in text or "crawler" in text:
            topics.append("ç½‘ç»œçˆ¬è™«")
        if "æ•°æ®åˆ†æ" in text or "data" in text:
            topics.append("æ•°æ®åˆ†æ")
        if "Hello World" in text or "hello" in text:
            topics.append("Hello Worldç¨‹åº")
        if "è®¾ç½®" in text:
            topics.append("ç³»ç»Ÿè®¾ç½®")
        if "è®°å¿†" in text or "è¯†åº•æ·±æ¹–" in text:
            topics.append("è®°å¿†ç³»ç»Ÿ")
        if "MCP" in text or "å·¥å…·" in text:
            topics.append("MCPå·¥å…·")
        if "æœç´¢" in text:
            topics.append("ç½‘ç»œæœç´¢")
        if "æ—¶é—´" in text:
            topics.append("æ—¶é—´æŸ¥è¯¢")
        # è‡ªæˆ‘ä»‹ç»ç›¸å…³ï¼ˆä¼˜å…ˆè¯†åˆ«ï¼‰
        if "æŒ‡æŒ¥å®˜ï¼Œæ‚¨å¥½ï¼æˆ‘æ˜¯éœ²å°¼è¥¿äºš" in text or "å¨å»‰çš„å§å§" in text:
            return "éœ²å°¼è¥¿äºšè‡ªæˆ‘ä»‹ç»"
        
        if "é—®å€™" in text or "ä½ å¥½" in text:
            topics.append("é—®å€™")
        if "ä»‹ç»" in text and any(country in text for country in ["å¾·å›½", "æ³•å›½", "è‹±å›½", "ç¾å›½", "æ—¥æœ¬", "éŸ©å›½", "ä¿„ç½—æ–¯", "ä¸­å›½", "å¡”æ—", "è´å°”æ ¼è±å¾·"]):
            topics.append("å›½å®¶ä»‹ç»")
        if "æ¸¸è®°" in text or "æ—…æ¸¸" in text or "è¡Œç¨‹" in text:
            topics.append("æ¸¸è®°å†™ä½œ")
        
        # æ ¹æ®å‘ç°çš„ä¸»é¢˜æ•°é‡ç”Ÿæˆç»¼åˆä¸»é¢˜
        if len(topics) >= 3:
            # å¤šä¸»é¢˜å¯¹è¯ï¼Œé€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªï¼Œé¿å…è¿‡äºå®½æ³›
            if "éŸ³ä¹æ¨è" in topics and "å¤©æ°”æŸ¥è¯¢" in topics:
                return f"{topics[0]}ä¸{topics[1]}ç­‰å¤šé¡¹è®¨è®º"
            else:
                # å¯¹äºå…¶ä»–å¤šä¸»é¢˜ï¼Œå°è¯•ç”Ÿæˆæ›´å…·ä½“çš„ä¸»é¢˜
                main_topics = topics[:3]  # å–å‰3ä¸ªä¸»é¢˜
                return f"{'ã€'.join(main_topics)}ç­‰å¤šé¡¹è®¨è®º"
        elif len(topics) == 2:
            # åŒä¸»é¢˜å¯¹è¯
            return f"{topics[0]}ä¸{topics[1]}è®¨è®º"
        elif len(topics) == 1:
            # å•ä¸»é¢˜å¯¹è¯
            return topics[0]
        else:
            # æ²¡æœ‰æ˜ç¡®ä¸»é¢˜ï¼Œå°è¯•æå–å…³é”®è¯
            keywords = self._extract_keywords(text)
            if keywords:
                return f"å…³äº{keywords[0]}çš„å¯¹è¯"
            else:
                return "æ—¥å¸¸å¯¹è¯"
                
    def _simple_summarize_content(self, text):
        """ç®€å•å†…å®¹æ€»ç»“"""
        summary_parts = []
        
        # æå–å…·ä½“ä¿¡æ¯
        if "ä½ å¥½" in text or "é—®å€™" in text:
            summary_parts.append("ç”¨æˆ·è¿›è¡Œäº†é—®å€™")
        
        if "å¤©æ°”" in text:
            # å°è¯•æå–åŸå¸‚ä¿¡æ¯å’Œå…·ä½“å¤©æ°”æ•°æ®
            cities = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æ­¦æ±‰", "æˆéƒ½", "é‡åº†", "è¥¿å®‰"]
            city_found = None
            for city in cities:
                if city in text:
                    city_found = city
                    break
            
            # å°è¯•æå–å…·ä½“çš„å¤©æ°”ä¿¡æ¯
            weather_details = []
            if "é›·é˜µé›¨" in text:
                weather_details.append("é›·é˜µé›¨")
            if "æ™´å¤©" in text or "æ™´" in text:
                weather_details.append("æ™´å¤©")
            if "å¤šäº‘" in text:
                weather_details.append("å¤šäº‘")
            if "é˜´" in text:
                weather_details.append("é˜´å¤©")
            if "é›¨" in text and "é›·é˜µé›¨" not in text:
                weather_details.append("é›¨å¤©")
            
            # å°è¯•æå–æ¸©åº¦ä¿¡æ¯
            import re
            temp_matches = re.findall(r'(\d+)Â°C', text)
            if temp_matches:
                if len(temp_matches) == 1:
                    weather_details.append(f"{temp_matches[0]}Â°C")
                else:
                    weather_details.append(f"{temp_matches[0]}-{temp_matches[-1]}Â°C")
            
            # å°è¯•æå–é£åŠ›ä¿¡æ¯
            wind_matches = re.findall(r'([ä¸œå—è¥¿åŒ—]é£\d+-\d+çº§)', text)
            if wind_matches:
                weather_details.append(wind_matches[0])
            
            # æ„å»ºå¤©æ°”æ€»ç»“
            if city_found and weather_details:
                summary_parts.append(f"æŸ¥è¯¢äº†{city_found}å¤©æ°”ï¼š{', '.join(weather_details[:3])}")
            elif city_found:
                summary_parts.append(f"æŸ¥è¯¢äº†{city_found}çš„å¤©æ°”ä¿¡æ¯")
            elif weather_details:
                summary_parts.append(f"æŸ¥è¯¢äº†å¤©æ°”ä¿¡æ¯ï¼š{', '.join(weather_details[:3])}")
            else:
                summary_parts.append("æŸ¥è¯¢äº†å¤©æ°”ä¿¡æ¯")
        
        if "æ—¶é—´" in text:
            summary_parts.append("æŸ¥è¯¢äº†å½“å‰æ—¶é—´")
        
        if "æœç´¢" in text:
            # å°è¯•æå–æœç´¢å…³é”®è¯
            import re
            search_match = re.search(r'æœç´¢\s*([^ï¼Œã€‚\s]+)', text)
            if search_match:
                keyword = search_match.group(1)
                summary_parts.append(f"æœç´¢äº†{keyword}ç›¸å…³ä¿¡æ¯")
            else:
                summary_parts.append("è¿›è¡Œäº†ç½‘ç»œæœç´¢")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯éŸ³ä¹æ¨èç›¸å…³çš„å¯¹è¯ï¼ˆéœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
        if ("æ¨è" in text and ("éŸ³ä¹" in text or "æ­Œå•" in text or "æ­Œæ›²" in text)) or \
           ("éŸ³ä¹" in text and ("æ¨è" in text or "å‡ é¦–" in text)):
            # å°è¯•æå–å…·ä½“çš„æ­Œæ›²ä¿¡æ¯
            import re
            # åŒ¹é…æ­Œæ›²åå­—ï¼ˆç”¨ã€Šã€‹åŒ…å›´çš„ï¼‰
            song_matches = re.findall(r'ã€Š([^ã€‹]+)ã€‹', text)
            if song_matches:
                songs = song_matches[:3]  # æœ€å¤šå–å‰3é¦–
                if len(songs) == 1:
                    summary_parts.append(f"æ¨èäº†éŸ³ä¹ã€Š{songs[0]}ã€‹")
                elif len(songs) == 2:
                    summary_parts.append(f"æ¨èäº†éŸ³ä¹ã€Š{songs[0]}ã€‹å’Œã€Š{songs[1]}ã€‹")
                else:
                    summary_parts.append(f"æ¨èäº†éŸ³ä¹ã€Š{songs[0]}ã€‹ç­‰{len(song_matches)}é¦–æ­Œæ›²")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ã€Šã€‹æ ¼å¼ï¼Œå°è¯•æå–å…¶ä»–æ ¼å¼çš„æ­Œæ›²å
                artist_matches = re.findall(r'-\s*([^ï¼ˆ\n]+)', text)
                if artist_matches:
                    artists = artist_matches[:2]  # æœ€å¤šå–å‰2ä¸ªè‰ºæœ¯å®¶
                    summary_parts.append(f"æ¨èäº†{artists[0]}ç­‰è‰ºæœ¯å®¶çš„éŸ³ä¹")
                else:
                    summary_parts.append("æ¨èäº†éŸ³ä¹æ­Œå•")
        
        if "Python" in text or "python" in text:
            # å°è¯•æå–å…·ä½“çš„Pythoné¡¹ç›®ä¿¡æ¯
            if "è®¡ç®—å™¨" in text:
                summary_parts.append("è®¨è®ºäº†Pythonè®¡ç®—å™¨ç¨‹åº")
            elif "ä¿„ç½—æ–¯æ–¹å—" in text or "tetris" in text:
                summary_parts.append("è®¨è®ºäº†Pythonä¿„ç½—æ–¯æ–¹å—æ¸¸æˆ")
            elif "è´ªåƒè›‡" in text or "snake" in text:
                summary_parts.append("è®¨è®ºäº†Pythonè´ªåƒè›‡æ¸¸æˆ")
            elif "äº•å­—æ£‹" in text or "tic-tac-toe" in text:
                summary_parts.append("è®¨è®ºäº†Pythonäº•å­—æ£‹æ¸¸æˆ")
            elif "çˆ¬è™«" in text or "crawler" in text:
                summary_parts.append("è®¨è®ºäº†Pythonç½‘ç»œçˆ¬è™«")
            elif "æ•°æ®åˆ†æ" in text or "data" in text:
                summary_parts.append("è®¨è®ºäº†Pythonæ•°æ®åˆ†æ")
            elif "Hello World" in text or "hello" in text:
                summary_parts.append("è®¨è®ºäº†Python Hello Worldç¨‹åº")
            else:
                summary_parts.append("è®¨è®ºäº†Pythonç¼–ç¨‹ç›¸å…³å†…å®¹")
        
        if "C++" in text or "c++" in text:
            # å°è¯•æå–å…·ä½“çš„C++é¡¹ç›®ä¿¡æ¯
            if "è®¡ç®—å™¨" in text:
                summary_parts.append("è®¨è®ºäº†C++è®¡ç®—å™¨ç¨‹åº")
            elif "ä¿„ç½—æ–¯æ–¹å—" in text or "tetris" in text:
                summary_parts.append("è®¨è®ºäº†C++ä¿„ç½—æ–¯æ–¹å—æ¸¸æˆ")
            elif "è´ªåƒè›‡" in text or "snake" in text:
                summary_parts.append("è®¨è®ºäº†C++è´ªåƒè›‡æ¸¸æˆ")
            elif "äº•å­—æ£‹" in text or "tic-tac-toe" in text:
                summary_parts.append("è®¨è®ºäº†C++äº•å­—æ£‹æ¸¸æˆ")
            else:
                summary_parts.append("è®¨è®ºäº†C++ç¼–ç¨‹ç›¸å…³å†…å®¹")
        
        if "Java" in text or "java" in text:
            # å°è¯•æå–å…·ä½“çš„Javaé¡¹ç›®ä¿¡æ¯
            if "è®¡ç®—å™¨" in text:
                summary_parts.append("è®¨è®ºäº†Javaè®¡ç®—å™¨ç¨‹åº")
            elif "ä¿„ç½—æ–¯æ–¹å—" in text or "tetris" in text:
                summary_parts.append("è®¨è®ºäº†Javaä¿„ç½—æ–¯æ–¹å—æ¸¸æˆ")
            elif "è´ªåƒè›‡" in text or "snake" in text:
                summary_parts.append("è®¨è®ºäº†Javaè´ªåƒè›‡æ¸¸æˆ")
            elif "äº•å­—æ£‹" in text or "tic-tac-toe" in text:
                summary_parts.append("è®¨è®ºäº†Javaäº•å­—æ£‹æ¸¸æˆ")
            else:
                summary_parts.append("è®¨è®ºäº†Javaç¼–ç¨‹ç›¸å…³å†…å®¹")
        
        if "COBOL" in text or "cobol" in text:
            summary_parts.append("è®¨è®ºäº†COBOLç¼–ç¨‹ç›¸å…³å†…å®¹")
        
        if "æ–‡ä»¶" in text and ("åˆ›å»º" in text or "ä¿å­˜" in text):
            # å°è¯•æå–å…·ä½“çš„æ–‡ä»¶ä¿¡æ¯
            import re
            # æå–æ–‡ä»¶ç±»å‹
            if ".py" in text or "Python" in text:
                summary_parts.append("åˆ›å»ºæˆ–ä¿å­˜äº†Pythonæ–‡ä»¶")
            elif ".cpp" in text or "C++" in text:
                summary_parts.append("åˆ›å»ºæˆ–ä¿å­˜äº†C++æ–‡ä»¶")
            elif ".java" in text or "Java" in text:
                summary_parts.append("åˆ›å»ºæˆ–ä¿å­˜äº†Javaæ–‡ä»¶")
            elif ".txt" in text:
                summary_parts.append("åˆ›å»ºæˆ–ä¿å­˜äº†æ–‡æœ¬æ–‡ä»¶")
            else:
                summary_parts.append("åˆ›å»ºæˆ–ä¿å­˜äº†æ–‡ä»¶")
        
        if "æ–‡ä»¶å¤¹" in text or "ç›®å½•" in text:
            summary_parts.append("åˆ›å»ºäº†æ–‡ä»¶å¤¹")
        
        # æ¸¸æˆå’Œé¡¹ç›®ç›¸å…³çš„æ€»ç»“å·²ç»åœ¨ç¼–ç¨‹éƒ¨åˆ†å¤„ç†äº†ï¼Œè¿™é‡Œä¸å†é‡å¤
        
        # æ£€æŸ¥è¯­è¨€ä»‹ç»ç›¸å…³çš„å¯¹è¯
        if "å¸Œä¼¯æ¥è¯­" in text or "ä¿„è¯­" in text or "è‹±è¯­" in text or "æ—¥è¯­" in text or "æ³•è¯­" in text or "å¾·è¯­" in text or "è¥¿ç­ç‰™è¯­" in text:
            if "ä»‹ç»" in text and "è‡ªå·±" in text:
                language = "å¸Œä¼¯æ¥è¯­" if "å¸Œä¼¯æ¥è¯­" in text else \
                          "ä¿„è¯­" if "ä¿„è¯­" in text else \
                          "è‹±è¯­" if "è‹±è¯­" in text else \
                          "æ—¥è¯­" if "æ—¥è¯­" in text else \
                          "æ³•è¯­" if "æ³•è¯­" in text else \
                          "å¾·è¯­" if "å¾·è¯­" in text else \
                          "è¥¿ç­ç‰™è¯­" if "è¥¿ç­ç‰™è¯­" in text else "å¤–è¯­"
                summary_parts.append(f"ç”¨{language}è¿›è¡Œäº†è‡ªæˆ‘ä»‹ç»")
            else:
                summary_parts.append("è¿›è¡Œäº†è¯­è¨€ç›¸å…³çš„å¯¹è¯")
        
        if "è®¾ç½®" in text:
            summary_parts.append("è¿›è¡Œäº†ç³»ç»Ÿè®¾ç½®ç›¸å…³æ“ä½œ")
        
        if "è®°å¿†" in text or "è¯†åº•æ·±æ¹–" in text:
            summary_parts.append("æŸ¥çœ‹äº†è®°å¿†ç³»ç»Ÿ")
        
        if "MCP" in text or "å·¥å…·" in text:
            summary_parts.append("ä½¿ç”¨äº†MCPå·¥å…·")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“å†…å®¹ï¼Œè¿”å›é€šç”¨æè¿°
        if not summary_parts:
            summary_parts.append("è¿›è¡Œäº†æ—¥å¸¸å¯¹è¯äº¤æµ")
        
        # ç»„åˆæ€»ç»“å†…å®¹ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
        if len(summary_parts) > 1:
            # å¦‚æœæœ‰å¤šä¸ªæ“ä½œï¼Œç”¨"ç„¶å"è¿æ¥ï¼Œè¡¨ç¤ºæ—¶é—´é¡ºåº
            summary = "ï¼Œç„¶å".join(summary_parts)
        else:
            summary = "ï¼Œ".join(summary_parts)
        
        # æ§åˆ¶é•¿åº¦åœ¨40-60å­—ä¹‹é—´
        if len(summary) > 60:
            summary = summary[:57] + "..."
        elif len(summary) < 25:
            summary += "ï¼ŒåŒ…å«å…·ä½“çš„å¯¹è¯å†…å®¹å’Œæ“ä½œæ­¥éª¤"
        
        return summary

    def _extract_keywords(self, text):
        """æå–å…³é”®è¯"""
        keywords = []
        common_words = [
            # åŸºç¡€åŠŸèƒ½
            'å¤©æ°”', 'æ—¶é—´', 'æœç´¢', 'æ‰“å¼€', 'è®¡ç®—', 'è·ç¦»', 'ç³»ç»Ÿ', 'æ–‡ä»¶', 'ç¬”è®°', 'ç©¿è¡£', 'å‡ºé—¨', 'å»ºè®®',
            # æ—…æ¸¸æ™¯ç‚¹
            'å†å²', 'æ™¯ç‚¹', 'æ—…æ¸¸', 'å‚è§‚', 'æ¸¸è§ˆ', 'å»ºç­‘', 'æ•™å ‚', 'å¤§æ•™å ‚', 'å¹¿åœº', 'å…¬å›­', 'åšç‰©é¦†', 'é—å€', 'å¤è¿¹',
            'æ•…å®«', 'å¤©å®‰é—¨', 'çº¢åœº', 'è«æ–¯ç§‘', 'æŸæ—', 'å‹ƒå…°ç™»å ¡é—¨', 'æ³•å…°å…‹ç¦', 'é“æ¡¥', 'æ¡¥',
            # ç¼–ç¨‹ç›¸å…³
            'Python', 'python', 'C++', 'c++', 'COBOL', 'cobol', 'ç¼–ç¨‹', 'ä»£ç ', 'ç¨‹åº', 'å¼€å‘',
            # æ–‡ä»¶æ“ä½œ
            'åˆ›å»º', 'ä¿å­˜', 'æ–‡ä»¶å¤¹', 'ç›®å½•', 'æ­Œå•', 'éŸ³ä¹', 'æ­Œæ›²', 'æ¨è',
            # æ¸¸æˆç›¸å…³
            'è®¡ç®—å™¨', 'ä¿„ç½—æ–¯æ–¹å—', 'tetris', 'è´ªåƒè›‡', 'snake', 'äº•å­—æ£‹', 'tic-tac-toe', 'æ¸¸æˆ',
            # æŠ€æœ¯ç›¸å…³
            'çˆ¬è™«', 'crawler', 'æ•°æ®åˆ†æ', 'data', 'Hello World', 'hello',
            # ç³»ç»ŸåŠŸèƒ½
            'è®¾ç½®', 'è®°å¿†', 'è¯†åº•æ·±æ¹–', 'MCP', 'å·¥å…·', 'API', 'é…ç½®'
        ]
        
        for word in common_words:
            if word in text:
                keywords.append(word)
        
        return keywords

    def _extract_conversation_details(self):
        """æå–å¯¹è¯è¯¦æƒ…ï¼Œç”Ÿæˆç²¾ç®€çš„å¯¹è¯è®°å½•"""
        if not self.current_conversation:
            return ""
        
        # ä½¿ç”¨AIæ™ºèƒ½æ€»ç»“æ•´ä¸ªå¯¹è¯ï¼Œè€Œä¸æ˜¯é€æ¡å…³é”®è¯è¯†åˆ«
        conversation_text = ""
        for conv in self.current_conversation:
            user_input = conv.get("user_input", "")
            ai_response = conv.get("ai_response", "")
            
            if user_input == "ç³»ç»Ÿ":
                conversation_text += f"éœ²å°¼è¥¿äºš: {ai_response}\n"
            else:
                conversation_text += f"æŒ‡æŒ¥å®˜: {user_input}\néœ²å°¼è¥¿äºš: {ai_response}\n"
        
        # å¼ºåˆ¶ä½¿ç”¨AIæ€»ç»“ï¼Œä¸å¯ç”¨åå¤‡æ–¹æ¡ˆ
        try:
            ai_result = self._ai_summarize_conversation_details(conversation_text)
            if ai_result and len(ai_result.strip()) > 10:  # ç¡®ä¿AIè¿”å›äº†æœ‰æ•ˆç»“æœ
                return ai_result
            else:
                print("âš ï¸ AIæ€»ç»“è¿”å›ç©ºç»“æœï¼Œå°è¯•é‡æ–°ç”Ÿæˆ")
                # å†æ¬¡å°è¯•AIæ€»ç»“
                ai_result = self._ai_summarize_conversation_details(conversation_text)
                return ai_result if ai_result and len(ai_result.strip()) > 10 else "AIæ€»ç»“å¤±è´¥"
        except Exception as e:
            print(f"âš ï¸ AIæ€»ç»“å¤±è´¥: {str(e)}")
            return "AIæ€»ç»“å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®"
    
    def _ai_summarize_conversation_details(self, conversation_text):
        """ä½¿ç”¨AIæ€»ç»“å¯¹è¯è¯¦æƒ…"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ å°è¯•AIå¯¹è¯è®°å½•æ€»ç»“ (ç¬¬{attempt + 1}æ¬¡)")
                # ä½¿ç”¨ä¸“é—¨çš„è®°å¿†æ€»ç»“AIä»£ç†
                details = self.summary_agent.summarize_conversation_details(conversation_text)
                if details and len(details.strip()) > 10:
                    print(f"âœ… AIå¯¹è¯è®°å½•æ€»ç»“æˆåŠŸ: {details[:50]}...")
                    return details
                else:
                    print(f"âš ï¸ AIå¯¹è¯è®°å½•æ€»ç»“è¿”å›ç©ºç»“æœ (ç¬¬{attempt + 1}æ¬¡)")
                    if attempt < max_retries - 1:
                        print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print("âŒ AIå¯¹è¯è®°å½•æ€»ç»“æœ€ç»ˆå¤±è´¥")
                        return "AIæ€»ç»“å¤±è´¥"
            except Exception as e:
                print(f"âš ï¸ AIå¯¹è¯è®°å½•æ€»ç»“å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {str(e)}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                    import time
                    time.sleep(2)
                    continue
                else:
                    print("âŒ AIå¯¹è¯è®°å½•æ€»ç»“æœ€ç»ˆå¤±è´¥")
                    return "AIæ€»ç»“å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®"
    
    def _fallback_conversation_details(self):
        """åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨åŸæ¥çš„å…³é”®è¯è¯†åˆ«æ–¹æ³•"""
        if not self.current_conversation:
            return ""
        
        details = []
        for conv in self.current_conversation:
            user_input = conv.get("user_input", "")
            ai_response = conv.get("ai_response", "")
            
            # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚è‡ªæˆ‘ä»‹ç»ï¼‰
            if user_input == "ç³»ç»Ÿ":
                details.append(f"éœ²å°¼è¥¿äºš: {ai_response}")
                continue
            
            # ç²¾ç®€ç”¨æˆ·è¾“å…¥
            if len(user_input) > 20:
                user_input = user_input[:17] + "..."
            
            # æ™ºèƒ½ç²¾ç®€AIå›åº”ï¼Œä¿ç•™å…·ä½“ä¿¡æ¯
            ai_response = self._smart_summarize_ai_response(ai_response)
            
            details.append(f"æŒ‡æŒ¥å®˜: {user_input}")
            details.append(f"éœ²å°¼è¥¿äºš: {ai_response}")
        
        return "\n".join(details)
    
    def _smart_summarize_ai_response(self, ai_response):
        """æ™ºèƒ½ç²¾ç®€AIå›åº”ï¼Œä¿ç•™å…·ä½“ä¿¡æ¯"""
        if len(ai_response) <= 50:
            return ai_response
        
        # è‡ªæˆ‘ä»‹ç»ç›¸å…³ï¼ˆä¼˜å…ˆäºéŸ³ä¹æ¨èï¼‰
        if "æŒ‡æŒ¥å®˜ï¼Œæ‚¨å¥½ï¼æˆ‘æ˜¯éœ²å°¼è¥¿äºš" in ai_response or "å¨å»‰çš„å§å§" in ai_response:
            return "è¿›è¡Œäº†è‡ªæˆ‘ä»‹ç»ï¼Œä»‹ç»äº†èº«ä»½å’Œèƒ½åŠ›"
        
        # éŸ³ä¹æ¨èç›¸å…³
        if "æ¨è" in ai_response and ("éŸ³ä¹" in ai_response or "æ­Œå•" in ai_response or "æ­Œæ›²" in ai_response):
            # æå–å…·ä½“çš„æ­Œæ›²ä¿¡æ¯
            import re
            song_matches = re.findall(r'ã€Š([^ã€‹]+)ã€‹', ai_response)
            if song_matches:
                # å®Œæ•´ç½—åˆ—æ‰€æœ‰æ­Œæ›²ï¼Œä½†æ§åˆ¶åœ¨200å­—ä»¥å†…
                if len(song_matches) <= 5:  # 5é¦–ä»¥å†…å®Œæ•´ç½—åˆ—
                    songs_text = "ã€".join([f"ã€Š{song}ã€‹" for song in song_matches])
                    return f"æ¨èäº†éŸ³ä¹{songs_text}"
                else:  # è¶…è¿‡5é¦–ï¼Œå‰5é¦–+æ€»æ•°
                    songs_text = "ã€".join([f"ã€Š{song}ã€‹" for song in song_matches[:5]])
                    return f"æ¨èäº†éŸ³ä¹{songs_text}ç­‰{len(song_matches)}é¦–æ­Œæ›²"
            else:
                # å°è¯•æå–è‰ºæœ¯å®¶ä¿¡æ¯
                artist_matches = re.findall(r'-\s*([^ï¼ˆ\n]+)', ai_response)
                if artist_matches:
                    artists = artist_matches[:3]  # æœ€å¤š3ä¸ªè‰ºæœ¯å®¶
                    artists_text = "ã€".join(artists)
                    return f"æ¨èäº†{artists_text}ç­‰è‰ºæœ¯å®¶çš„éŸ³ä¹"
                else:
                    return "æ¨èäº†éŸ³ä¹æ­Œå•"
        
        # å›½å®¶ä»‹ç»å’Œç§‘æ™®å†…å®¹ç›¸å…³ï¼ˆä¼˜å…ˆäºå¤©æ°”ä¿¡æ¯ï¼‰
        elif any(keyword in ai_response for keyword in ["å¾·å›½", "æ³•å›½", "è‹±å›½", "ç¾å›½", "æ—¥æœ¬", "éŸ©å›½", "ä¿„ç½—æ–¯", "ä¸­å›½", "ä»‹ç»", "ä½äº", "é¦–éƒ½", "äººå£", "é¢ç§¯", "ç»æµ", "æ–‡åŒ–", "å†å²"]):
            # æå–å›½å®¶æˆ–åœ°åŒºåç§°
            import re
            country_match = re.search(r'([å¾·å›½æ³•å›½è‹±å›½ç¾å›½æ—¥æœ¬éŸ©å›½ä¿„ç½—æ–¯ä¸­å›½å°åº¦å·´è¥¿æ¾³å¤§åˆ©äºšåŠ æ‹¿å¤§æ„å¤§åˆ©è¥¿ç­ç‰™è·å…°ç‘å£«ç‘å…¸æŒªå¨ä¸¹éº¦èŠ¬å…°æ³¢å…°æ·å…‹åŒˆç‰™åˆ©ç½—é©¬å°¼äºšä¿åŠ åˆ©äºšå¡å°”ç»´äºšå…‹ç½—åœ°äºšæ–¯æ´›æ–‡å°¼äºšå¥¥åœ°åˆ©æ¯”åˆ©æ—¶å¢æ£®å ¡è‘¡è„ç‰™å¸Œè…ŠåœŸè€³å…¶ä»¥è‰²åˆ—åŸƒåŠå—éå°¼æ—¥åˆ©äºšè‚¯å°¼äºšåŸƒå¡ä¿„æ¯”äºšæ‘©æ´›å“¥é˜¿å°”åŠåˆ©äºšçªå°¼æ–¯åˆ©æ¯”äºšè‹ä¸¹å—è‹ä¸¹ä¸­éå…±å’Œå›½åˆšæœæ°‘ä¸»å…±å’Œå›½åˆšæœå…±å’Œå›½åŠ è“¬èµ¤é“å‡ å†…äºšåœ£å¤šç¾å’Œæ™®æ—è¥¿æ¯”å–€éº¦éš†ä¹å¾—å°¼æ—¥å°”é©¬é‡Œå¸ƒåŸºçº³æ³•ç´¢è´å®å¤šå“¥åŠ çº³ç§‘ç‰¹è¿ªç“¦åˆ©æ¯”é‡Œäºšå¡æ‹‰åˆ©æ˜‚å‡ å†…äºšå‡ å†…äºšæ¯”ç»å¡å†…åŠ å°”å†ˆæ¯”äºšæ¯›é‡Œå¡”å°¼äºšæ‘©æ´›å“¥é˜¿å°”åŠåˆ©äºšçªå°¼æ–¯åˆ©æ¯”äºšåŸƒåŠè‹ä¸¹å—è‹ä¸¹ä¸­éå…±å’Œå›½åˆšæœæ°‘ä¸»å…±å’Œå›½åˆšæœå…±å’Œå›½åŠ è“¬èµ¤é“å‡ å†…äºšåœ£å¤šç¾å’Œæ™®æ—è¥¿æ¯”å–€éº¦éš†ä¹å¾—å°¼æ—¥å°”é©¬é‡Œå¸ƒåŸºçº³æ³•ç´¢è´å®å¤šå“¥åŠ çº³ç§‘ç‰¹è¿ªç“¦åˆ©æ¯”é‡Œäºšå¡æ‹‰åˆ©æ˜‚å‡ å†…äºšå‡ å†…äºšæ¯”ç»å¡å†…åŠ å°”å†ˆæ¯”äºšæ¯›é‡Œå¡”å°¼äºš])(å›½|å…±å’Œå›½|è”é‚¦|ç‹å›½|å¸å›½|å…¬å›½|å¤§å…¬å›½|é…‹é•¿å›½|è‹ä¸¹å›½|å“ˆé‡Œå‘å›½|å…±å’Œå›½|è”é‚¦å…±å’Œå›½|æ°‘ä¸»å…±å’Œå›½|äººæ°‘å…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|ä¼Šæ–¯å…°å…±å’Œå›½|é˜¿æ‹‰ä¼¯å…±å’Œå›½|è”åˆå…±å’Œå›½|è”é‚¦å…±å’Œå›½|æ°‘ä¸»è”é‚¦å…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰è”é‚¦å…±å’Œå›½|ä¼Šæ–¯å…°è”é‚¦å…±å’Œå›½|é˜¿æ‹‰ä¼¯è”é‚¦å…±å’Œå›½|è”åˆè”é‚¦å…±å’Œå›½|è”é‚¦æ°‘ä¸»å…±å’Œå›½|è”é‚¦ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|è”é‚¦ä¼Šæ–¯å…°å…±å’Œå›½|è”é‚¦é˜¿æ‹‰ä¼¯å…±å’Œå›½|è”é‚¦è”åˆå…±å’Œå›½|æ°‘ä¸»è”é‚¦ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|æ°‘ä¸»è”é‚¦ä¼Šæ–¯å…°å…±å’Œå›½|æ°‘ä¸»è”é‚¦é˜¿æ‹‰ä¼¯å…±å’Œå›½|æ°‘ä¸»è”é‚¦è”åˆå…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰è”é‚¦æ°‘ä¸»å…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰è”é‚¦ä¼Šæ–¯å…°å…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰è”é‚¦é˜¿æ‹‰ä¼¯å…±å’Œå›½|ç¤¾ä¼šä¸»ä¹‰è”é‚¦è”åˆå…±å’Œå›½|ä¼Šæ–¯å…°è”é‚¦æ°‘ä¸»å…±å’Œå›½|ä¼Šæ–¯å…°è”é‚¦ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|ä¼Šæ–¯å…°è”é‚¦é˜¿æ‹‰ä¼¯å…±å’Œå›½|ä¼Šæ–¯å…°è”é‚¦è”åˆå…±å’Œå›½|é˜¿æ‹‰ä¼¯è”é‚¦æ°‘ä¸»å…±å’Œå›½|é˜¿æ‹‰ä¼¯è”é‚¦ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|é˜¿æ‹‰ä¼¯è”é‚¦ä¼Šæ–¯å…°å…±å’Œå›½|é˜¿æ‹‰ä¼¯è”é‚¦è”åˆå…±å’Œå›½|è”åˆè”é‚¦æ°‘ä¸»å…±å’Œå›½|è”åˆè”é‚¦ç¤¾ä¼šä¸»ä¹‰å…±å’Œå›½|è”åˆè”é‚¦ä¼Šæ–¯å…°å…±å’Œå›½|è”åˆè”é‚¦é˜¿æ‹‰ä¼¯å…±å’Œå›½|è”åˆè”é‚¦è”åˆå…±å’Œå›½)?', ai_response)
            if country_match:
                country = country_match.group(1)
                # æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç¼©å†™å¥å­
                summary_parts = []
                
                # æå–åœ°ç†ä½ç½®
                if "ä½äº" in ai_response:
                    location_match = re.search(r'ä½äº([^ï¼Œã€‚\s]+)', ai_response)
                    if location_match:
                        summary_parts.append(f"ä½äº{location_match.group(1)}")
                
                # æå–é¦–éƒ½
                if "é¦–éƒ½" in ai_response:
                    capital_match = re.search(r'é¦–éƒ½([^ï¼Œã€‚\s]+)', ai_response)
                    if capital_match:
                        summary_parts.append(f"é¦–éƒ½{capital_match.group(1)}")
                
                # æå–äººå£
                if "äººå£" in ai_response:
                    population_match = re.search(r'äººå£([^ï¼Œã€‚\s]+)', ai_response)
                    if population_match:
                        summary_parts.append(f"äººå£{population_match.group(1)}")
                
                # æå–é¢ç§¯
                if "é¢ç§¯" in ai_response:
                    area_match = re.search(r'é¢ç§¯([^ï¼Œã€‚\s]+)', ai_response)
                    if area_match:
                        summary_parts.append(f"é¢ç§¯{area_match.group(1)}")
                
                # æ„å»ºæ€»ç»“
                if summary_parts:
                    return f"ä»‹ç»äº†{country}ï¼š{''.join(summary_parts[:3])}"  # æœ€å¤š3ä¸ªå…³é”®ä¿¡æ¯
                else:
                    return f"ä»‹ç»äº†{country}çš„åŸºæœ¬ä¿¡æ¯"
            else:
                # æ²¡æœ‰æ‰¾åˆ°å…·ä½“å›½å®¶ï¼Œä½†åŒ…å«ä»‹ç»ç›¸å…³å†…å®¹
                if "ä»‹ç»" in ai_response:
                    return "è¿›è¡Œäº†çŸ¥è¯†ä»‹ç»"
                else:
                    return "æä¾›äº†ç§‘æ™®ä¿¡æ¯"
        
        # å¤©æ°”æŸ¥è¯¢ç›¸å…³
        elif "å¤©æ°”" in ai_response:
            # æå–å…·ä½“çš„å¤©æ°”ä¿¡æ¯
            import re
            weather_details = []
            
            # æå–åŸå¸‚ä¿¡æ¯
            city_match = re.search(r'([åŒ—äº¬ä¸Šæµ·å¹¿å·æ·±åœ³æˆéƒ½é‡åº†æ­¦æ±‰è¥¿å®‰å—äº¬æ­å·è‹å·å¤©æ´¥é’å²›å¤§è¿å¦é—¨å®æ³¢æ— é”¡é•¿æ²™éƒ‘å·æµå—ç¦å·åˆè‚¥å—æ˜Œå—å®è´µé˜³æ˜†æ˜å¤ªåŸçŸ³å®¶åº„å“ˆå°”æ»¨é•¿æ˜¥æ²ˆé˜³å‘¼å’Œæµ©ç‰¹é“¶å·è¥¿å®æ‹‰è¨ä¹Œé²æœ¨é½])(å¸‚|çœ)?', ai_response)
            if city_match:
                city = city_match.group(1)
                weather_details.append(city)
            
            # æå–æ¸©åº¦ä¿¡æ¯
            temp_matches = re.findall(r'(\d+)Â°C', ai_response)
            if temp_matches:
                if len(temp_matches) == 1:
                    weather_details.append(f"{temp_matches[0]}Â°C")
                else:
                    weather_details.append(f"{temp_matches[0]}-{temp_matches[-1]}Â°C")
            
            # æå–å¤©æ°”çŠ¶å†µ
            if "é›·é˜µé›¨" in ai_response:
                weather_details.append("é›·é˜µé›¨")
            elif "å¤šäº‘" in ai_response:
                weather_details.append("å¤šäº‘")
            elif "æ™´å¤©" in ai_response:
                weather_details.append("æ™´å¤©")
            elif "é˜´å¤©" in ai_response:
                weather_details.append("é˜´å¤©")
            elif "å°é›¨" in ai_response:
                weather_details.append("å°é›¨")
            
            # æå–é£åŠ›ä¿¡æ¯
            wind_matches = re.findall(r'([ä¸œå—è¥¿åŒ—]é£\d+-\d+çº§)', ai_response)
            if wind_matches:
                weather_details.append(wind_matches[0])
            
            # æ„å»ºå¤©æ°”æ€»ç»“
            if weather_details:
                return f"æä¾›äº†{''.join(weather_details)}çš„å¤©æ°”ä¿¡æ¯"
            else:
                return "æä¾›äº†å¤©æ°”ä¿¡æ¯"
        
        # æ–‡ä»¶æ“ä½œç›¸å…³
        elif "æ–‡ä»¶" in ai_response and ("æˆåŠŸ" in ai_response or "å†™å…¥æˆåŠŸ" in ai_response):
            # æå–æ–‡ä»¶è·¯å¾„å’Œç±»å‹
            import re
            file_match = re.search(r'æ–‡ä»¶\s*([^å†™å…¥æˆåŠŸ]+)', ai_response)
            if file_match:
                file_path = file_match.group(1).strip()
                return f"æ–‡ä»¶{file_path}åˆ›å»ºæˆåŠŸ"
            else:
                return "æ–‡ä»¶åˆ›å»ºæˆåŠŸ"
        
        # æ—¶é—´æŸ¥è¯¢ç›¸å…³
        elif "æ—¶é—´" in ai_response:
            # æå–å…·ä½“æ—¶é—´ä¿¡æ¯
            import re
            time_match = re.search(r'(\d{1,2}:\d{2})', ai_response)
            if time_match:
                time_str = time_match.group(1)
                return f"æä¾›äº†{time_str}çš„æ—¶é—´ä¿¡æ¯"
            else:
                return "æä¾›äº†æ—¶é—´ä¿¡æ¯"
        
        # ç¼–ç¨‹ç›¸å…³
        elif any(keyword in ai_response for keyword in ["Python", "Java", "C++", "JavaScript", "ä»£ç ", "ç¨‹åº"]):
            # æå–ç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®ç±»å‹
            import re
            if "Python" in ai_response:
                if "è®¡ç®—å™¨" in ai_response:
                    return "æä¾›äº†Pythonè®¡ç®—å™¨ä»£ç "
                elif "ä¿„ç½—æ–¯æ–¹å—" in ai_response or "tetris" in ai_response:
                    return "æä¾›äº†Pythonä¿„ç½—æ–¯æ–¹å—æ¸¸æˆä»£ç "
                elif "è´ªåƒè›‡" in ai_response or "snake" in ai_response:
                    return "æä¾›äº†Pythonè´ªåƒè›‡æ¸¸æˆä»£ç "
                else:
                    return "æä¾›äº†Pythonç¼–ç¨‹ä»£ç "
            elif "Java" in ai_response:
                if "è®¡ç®—å™¨" in ai_response:
                    return "æä¾›äº†Javaè®¡ç®—å™¨ä»£ç "
                elif "æ¸¸æˆ" in ai_response:
                    return "æä¾›äº†Javaæ¸¸æˆä»£ç "
                else:
                    return "æä¾›äº†Javaç¼–ç¨‹ä»£ç "
            elif "C++" in ai_response:
                if "æ¸¸æˆ" in ai_response:
                    return "æä¾›äº†C++æ¸¸æˆä»£ç "
                else:
                    return "æä¾›äº†C++ç¼–ç¨‹ä»£ç "
            else:
                return "æä¾›äº†ç¼–ç¨‹ä»£ç "
        
        # è¯­è¨€ä»‹ç»ç›¸å…³
        elif any(lang in ai_response for lang in ["å¸Œä¼¯æ¥è¯­", "ä¿„è¯­", "è‹±è¯­", "æ—¥è¯­", "æ³•è¯­", "å¾·è¯­", "è¥¿ç­ç‰™è¯­"]):
            for lang in ["å¸Œä¼¯æ¥è¯­", "ä¿„è¯­", "è‹±è¯­", "æ—¥è¯­", "æ³•è¯­", "å¾·è¯­", "è¥¿ç­ç‰™è¯­"]:
                if lang in ai_response:
                    return f"ç”¨{lang}è¿›è¡Œäº†è‡ªæˆ‘ä»‹ç»"
            return "è¿›è¡Œäº†è¯­è¨€ä»‹ç»"
        
        # å…¶ä»–æƒ…å†µï¼Œä¿ç•™å…³é”®ä¿¡æ¯
        else:
            # å°è¯•æå–å…³é”®ä¿¡æ¯ï¼Œé¿å…è¿‡é•¿
            if len(ai_response) > 100:
                # å¯»æ‰¾å¥å·æˆ–é€—å·ä½œä¸ºåˆ†å‰²ç‚¹
                sentences = ai_response.split('ã€‚')
                if len(sentences) > 1:
                    first_sentence = sentences[0].strip()
                    if len(first_sentence) <= 50:
                        return first_sentence
                    else:
                        return first_sentence[:47] + "..."
                else:
                    return ai_response[:47] + "..."
            else:
                return ai_response

    def search_relevant_memories(self, user_input, current_context=""):
        """æœç´¢ç›¸å…³è®°å¿†ï¼ˆå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼‰"""
        return self._search_by_vectors(user_input, current_context)
    
    def _search_by_vectors(self, user_input, current_context=""):
        """ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢ç›¸å…³è®°å¿†"""
        try:
            print(f"ğŸ—„ï¸ å¼€å§‹å‘é‡æœç´¢è®°å¿†: {user_input[:50]}...")
            
            # ç”Ÿæˆç”¨æˆ·è¾“å…¥çš„å‘é‡
            user_vector = self.vector_encoder.encode_text(user_input)
            
            if user_vector is None:
                print("âš ï¸ æ— æ³•ç”Ÿæˆç”¨æˆ·è¾“å…¥çš„å‘é‡ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
                return self._search_by_keywords(user_input, current_context)
            
            relevant_memories = []
            
            for entry in self.memory_index["topics"]:
                if "topic_vector" in entry and entry["topic_vector"]:
                    # è®¡ç®—ä¸»é¢˜å‘é‡ç›¸ä¼¼åº¦
                    topic_similarity = self.vector_encoder.calculate_similarity(
                        user_vector, entry["topic_vector"]
                    )
                    
                    # è®¡ç®—å†…å®¹å‘é‡ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    details_similarity = 0.0
                    if "details_vector" in entry and entry["details_vector"]:
                        details_similarity = self.vector_encoder.calculate_similarity(
                            user_vector, entry["details_vector"]
                        )
                    
                    # ç»¼åˆç›¸ä¼¼åº¦ï¼šä¸»é¢˜æƒé‡70%ï¼Œå†…å®¹æƒé‡30%
                    combined_similarity = topic_similarity * 0.7 + details_similarity * 0.3
                    
                    if combined_similarity > 0.2 or details_similarity > 0.3:  # åŒé‡é˜ˆå€¼
                        entry_copy = entry.copy()
                        entry_copy["relevance_score"] = combined_similarity
                        entry_copy["topic_similarity"] = topic_similarity
                        entry_copy["details_similarity"] = details_similarity
                        relevant_memories.append(entry_copy)
                else:
                    # å¯¹äºæ²¡æœ‰å‘é‡çš„è®°å¿†ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå¤‡ç”¨
                    keyword_score = self._calculate_keyword_relevance(entry, user_input, current_context)
                    if keyword_score > 0.3:
                        entry_copy = entry.copy()
                        entry_copy["relevance_score"] = keyword_score * 0.5  # é™ä½æƒé‡
                        relevant_memories.append(entry_copy)
            
            # æŒ‰ç›¸å…³æ€§æ’åºï¼Œç„¶åæŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„ä¼˜å…ˆï¼‰
            relevant_memories.sort(key=lambda x: (-x["relevance_score"], -self._get_timestamp_score(x)))
            
            # é™åˆ¶è¿”å›æ•°é‡
            max_memories = 5
            relevant_memories = relevant_memories[:max_memories]
            
            if relevant_memories:
                print(f"ğŸ¯ æ‰¾åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿† (åŒé‡å‘é‡æœç´¢)")
                for i, memory in enumerate(relevant_memories):
                    score = memory["relevance_score"]
                    topic_sim = memory.get("topic_similarity", 0)
                    details_sim = memory.get("details_similarity", 0)
                    topic = memory.get("topic", "æœªçŸ¥ä¸»é¢˜")[:30]
                    print(f"   {i+1}. [æ€»:{score:.3f}|ä¸»é¢˜:{topic_sim:.3f}|å†…å®¹:{details_sim:.3f}] {topic}")
            else:
                print("ğŸ” æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œå°è¯•å…³é”®è¯æœç´¢...")
                return self._search_by_keywords(user_input, current_context)
            
            return relevant_memories
            
        except Exception as e:
            print(f"âš ï¸ å‘é‡æœç´¢å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°å…³é”®è¯æœç´¢...")
            return self._search_by_keywords(user_input, current_context)
    
    
    def _search_by_keywords(self, user_input, current_context=""):
        """å…³é”®è¯æœç´¢å¤‡ç”¨æ–¹æ³•"""
        try:
            print(f"ğŸ” å¼€å§‹å…³é”®è¯æœç´¢è®°å¿†: {user_input[:50]}...")
            relevant_memories = []
            user_keywords = self._extract_keywords(user_input)
            
            for entry in self.memory_index["topics"]:
                relevance_score = self._calculate_keyword_relevance(entry, user_input, current_context)
                if relevance_score > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                    entry_copy = entry.copy()
                    entry_copy["relevance_score"] = relevance_score
                    relevant_memories.append(entry_copy)
            
            # æŒ‰ç›¸å…³æ€§æ’åºï¼Œç„¶åæŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„ä¼˜å…ˆï¼‰
            relevant_memories.sort(key=lambda x: (-x["relevance_score"], -self._get_timestamp_score(x)))
            
            if relevant_memories:
                print(f"ğŸ¯ æ‰¾åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿† (å…³é”®è¯æœç´¢)")
                for i, memory in enumerate(relevant_memories[:5]):
                    score = memory["relevance_score"]
                    topic = memory.get("topic", "æœªçŸ¥ä¸»é¢˜")[:30]
                    print(f"   {i+1}. [{score:.3f}] {topic}")
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
            
            return relevant_memories[:5]  # è¿”å›æœ€ç›¸å…³çš„5ä¸ªè®°å¿†
            
        except Exception as e:
            print(f"âš ï¸ å…³é”®è¯æœç´¢å¤±è´¥: {str(e)}")
            return []

    def _calculate_keyword_relevance(self, memory_entry, user_keywords, current_context):
        """è®¡ç®—å…³é”®è¯ç›¸å…³æ€§åˆ†æ•°ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…
        memory_keywords = memory_entry.get("keywords", [])
        for keyword in user_keywords:
            if keyword in memory_keywords:
                score += 0.4
        
        # ä¸»é¢˜åŒ¹é…
        memory_topic = memory_entry.get("topic", "")
        for keyword in user_keywords:
            if keyword in memory_topic:
                score += 0.3
        
        # æ—¶é—´ç›¸å…³æ€§ï¼ˆæœ€è¿‘7å¤©çš„è®°å¿†æƒé‡æ›´é«˜ï¼‰
        try:
            memory_date = datetime.datetime.strptime(memory_entry.get("date", ""), "%Y-%m-%d")
            current_date = datetime.datetime.now()
            days_diff = (current_date - memory_date).days
            if days_diff <= 7:
                score += 0.2
            elif days_diff <= 30:
                score += 0.1
        except:
            pass
        
        return min(score, 1.0)

    def should_recall_memory(self, user_input):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å›å¿†"""
        # å…³é”®è¯è§¦å‘ - æ›´ç²¾ç¡®çš„å…³é”®è¯
        recall_keywords = ['è®°å¾—', 'è¯´è¿‡', 'è®¨è®ºè¿‡', 'å›å¿†', 'ç»§ç»­', 'æ¥ç€', 'å†å²', 'ä»¥å‰', 'æ›¾ç»', 'ä¹‹å‰', 'ä¸Šä¸ª']
        
        # å¦‚æœç”¨æˆ·è¯¢é—®çš„æ˜¯"ä¸Šä¸€ä¸ª"ç›¸å…³çš„é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬æ¬¡ä¼šè¯è®°å¿†ï¼Œä¸è§¦å‘å†å²è®°å¿†
        # ä½†å¦‚æœæ˜¯"ä¹‹å‰"ç›¸å…³çš„é—®é¢˜ï¼Œåº”è¯¥è§¦å‘å†å²è®°å¿†
        if any(word in user_input for word in ['ä¸Šä¸€ä¸ª', 'åˆšæ‰']):
            return False
            
        return any(keyword in user_input for keyword in recall_keywords)

    def generate_memory_context(self, relevant_memories, user_input):
        """ç”Ÿæˆè®°å¿†ä¸Šä¸‹æ–‡"""
        if not relevant_memories:
            return ""
            
        try:
            context_parts = []
            
            for memory in relevant_memories:
                topic = memory.get("topic", "")
                timestamp = memory.get("timestamp", "")
                date = memory.get("date", "")
                
                context_part = f"ã€{date} {timestamp}ã€‘{topic}"
                context_parts.append(context_part)
            
            if context_parts:
                return "\n".join(context_parts)
            
            return ""
            
        except Exception as e:
            print(f"ç”Ÿæˆè®°å¿†ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return ""

    def get_recent_memories(self, limit=100):
        """è·å–æœ€è¿‘çš„å†å²è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            # æŒ‰æ—¥æœŸå’Œæ—¶é—´å€’åºæ’åˆ—ï¼Œè·å–æœ€è¿‘çš„è®°å¿†
            sorted_topics = sorted(topics, key=lambda x: (x.get("date", ""), x.get("timestamp", "")), reverse=True)
            return sorted_topics[:limit]
        except Exception as e:
            print(f"è·å–æœ€è¿‘è®°å¿†å¤±è´¥: {str(e)}")
            return []

    def get_first_memory(self):
        """è·å–ç¬¬ä¸€æ¡è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            if not topics:
                return None
        
            # æŒ‰æ—¥æœŸå’Œæ—¶é—´æ­£åºæ’åˆ—ï¼Œè·å–æœ€æ—©çš„è®°å¿†
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®ï¼Œå¤„ç†å¯èƒ½çš„ç©ºå€¼
            def sort_key(topic):
                date = topic.get("date", "")
                timestamp = topic.get("timestamp", "")
                # å¦‚æœæ—¥æœŸä¸ºç©ºï¼Œä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„æ—¥æœŸç¡®ä¿æ’åœ¨æœ€å
                if not date:
                    return ("9999-12-31", timestamp)
                return (date, timestamp)
            
            sorted_topics = sorted(topics, key=sort_key)
            first_memory = sorted_topics[0]
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” æ‰¾åˆ°ç¬¬ä¸€æ¡è®°å¿†: {first_memory.get('date', 'æœªçŸ¥')} {first_memory.get('timestamp', 'æœªçŸ¥')} - {first_memory.get('topic', 'æœªçŸ¥ä¸»é¢˜')}")
            
            return first_memory
        except Exception as e:
            print(f"è·å–ç¬¬ä¸€æ¡è®°å¿†å¤±è´¥: {str(e)}")
            return None

    def get_memory_stats(self):
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            topics = self.memory_index.get("topics", [])
            total_topics = len(topics)
            important_topics = len([topic for topic in topics if topic.get("is_important", False)])
            total_log_files = len([f for f in os.listdir(self.chat_logs_dir) if f.endswith('.json')]) if os.path.exists(self.chat_logs_dir) else 0
            
            stats = {
                "total_topics": total_topics,
                "important_topics": important_topics,
                "total_log_files": total_log_files,
                "memory_file_size": os.path.getsize(self.memory_file) if os.path.exists(self.memory_file) else 0,
                "current_conversation_count": len(self.current_conversation)
            }
            
            return stats
        except Exception as e:
            print(f"è·å–è®°å¿†ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {"total_topics": 0, "important_topics": 0, "total_log_files": 0, "memory_file_size": 0, "current_conversation_count": 0}
    

    def mark_as_important(self, topic_index):
        """æ ‡è®°ä¸ºé‡ç‚¹è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            if 0 <= topic_index < len(topics):
                topics[topic_index]["is_important"] = True
                self.save_memory()
                return True
            return False
        except Exception as e:
            print(f"æ ‡è®°é‡ç‚¹è®°å¿†å¤±è´¥: {str(e)}")
            return False

    def unmark_as_important(self, topic_index):
        """å–æ¶ˆé‡ç‚¹è®°å¿†æ ‡è®°"""
        try:
            topics = self.memory_index.get("topics", [])
            if 0 <= topic_index < len(topics):
                topics[topic_index]["is_important"] = False
                self.save_memory()
                return True
            return False
        except Exception as e:
            print(f"å–æ¶ˆé‡ç‚¹è®°å¿†æ ‡è®°å¤±è´¥: {str(e)}")
            return False

    def get_important_memories(self):
        """è·å–æ‰€æœ‰é‡ç‚¹è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            important_memories = [topic for topic in topics if topic.get("is_important", False)]
            return important_memories
        except Exception as e:
            print(f"è·å–é‡ç‚¹è®°å¿†å¤±è´¥: {str(e)}")
            return []

    def mark_first_memory_as_important(self):
        """å°†ç¬¬ä¸€æ¡è®°å¿†æ ‡è®°ä¸ºé‡ç‚¹è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            if topics:
                topics[0]["is_important"] = True
                self.save_memory()
                return True
            return False
        except Exception as e:
            print(f"æ ‡è®°ç¬¬ä¸€æ¡è®°å¿†ä¸ºé‡ç‚¹è®°å¿†å¤±è´¥: {str(e)}")
            return False

    def ensure_first_memory_important(self):
        """ç¡®ä¿ç¬¬ä¸€æ¡è®°å¿†æ˜¯é‡ç‚¹è®°å¿†"""
        try:
            topics = self.memory_index.get("topics", [])
            if topics and not topics[0].get("is_important", False):
                topics[0]["is_important"] = True
                self.save_memory()
                return True
            return False
        except Exception as e:
            print(f"ç¡®ä¿ç¬¬ä¸€æ¡è®°å¿†ä¸ºé‡ç‚¹è®°å¿†å¤±è´¥: {str(e)}")
            return False
