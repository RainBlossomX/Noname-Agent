{
  "æ™ºèƒ½æ–‡ä»¶åˆ†æ": {
    "description": "æ™ºèƒ½æ–‡ä»¶åˆ†æå·¥å…·ï¼Œæ”¯æŒå›¾ç‰‡å†…å®¹è¯†åˆ«ã€æ–‡æ¡£å†…å®¹åˆ†æã€AIæ·±åº¦ç†è§£",
    "code": "import os\nimport json\nimport base64\nimport mimetypes\nimport hashlib\nimport datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\n\ntry:\n    from PIL import Image\n    PIL_AVAILABLE = True\nexcept ImportError:\n    PIL_AVAILABLE = False\n\ntry:\n    import fitz  # PyMuPDF\n    PDF_AVAILABLE = True\nexcept ImportError:\n    PDF_AVAILABLE = False\n\ntry:\n    import pandas as pd\n    PANDAS_AVAILABLE = True\nexcept ImportError:\n    PANDAS_AVAILABLE = False\n\ntry:\n    import pytesseract\n    from PIL import ImageEnhance\n    # è®¾ç½®Tesseractè·¯å¾„\n    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n    OCR_AVAILABLE = True\nexcept ImportError:\n    OCR_AVAILABLE = False\n\ndef analyze_file_content(file_path):\n    \"\"\"æ·±åº¦åˆ†ææ–‡ä»¶å†…å®¹ï¼ŒåŒ…æ‹¬AIç†è§£å’Œå†…å®¹æå–\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            return f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\"\n        \n        file_path = Path(file_path)\n        \n        # è·å–åŸºæœ¬ä¿¡æ¯\n        basic_info = get_basic_file_info(file_path)\n        \n        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œå†…å®¹åˆ†æ\n        if is_image_file(file_path):\n            content_analysis = analyze_image_content(file_path)\n        elif is_document_file(file_path):\n            content_analysis = analyze_document_content(file_path)\n        else:\n            content_analysis = {\"type\": \"unknown\", \"message\": \"æš‚ä¸æ”¯æŒæ­¤æ–‡ä»¶ç±»å‹çš„å†…å®¹åˆ†æ\"}\n        \n        # åˆå¹¶åˆ†æç»“æœ\n        result = {\n            \"basic_info\": basic_info,\n            \"content_analysis\": content_analysis,\n            \"analysis_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        \n        return json.dumps(result, ensure_ascii=False, indent=2)\n        \n    except Exception as e:\n        return f\"æ–‡ä»¶å†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n\ndef analyze_image_content(file_path):\n    \"\"\"åˆ†æå›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯è¯†åˆ«ã€ç‰©ä½“æ£€æµ‹ã€æ–‡å­—æå–ç­‰\"\"\"\n    if not PIL_AVAILABLE:\n        return {\n            \"type\": \"image\",\n            \"error\": \"PILåº“æœªå®‰è£…ï¼Œæ— æ³•åˆ†æå›¾ç‰‡å†…å®¹\",\n            \"suggestion\": \"è¯·å®‰è£…Pillowåº“: pip install Pillow\"\n        }\n    \n    try:\n        with Image.open(file_path) as img:\n            # åŸºç¡€å›¾ç‰‡ä¿¡æ¯\n            basic_info = {\n                \"type\": \"image\",\n                \"format\": img.format,\n                \"mode\": img.mode,\n                \"width\": img.width,\n                \"height\": img.height,\n                \"aspect_ratio\": round(img.width / img.height, 2),\n                \"color_depth\": get_color_depth(img.mode)\n            }\n            \n            # å›¾ç‰‡å†…å®¹åˆ†æ\n            content_analysis = {\n                \"scene_description\": analyze_image_scene(img),\n                \"object_detection\": detect_objects_in_image(img),\n                \"text_extraction\": extract_text_from_image(img),\n                \"ocr_text\": perform_ocr_on_image(img),\n                \"color_analysis\": analyze_image_colors(img),\n                \"composition_analysis\": analyze_image_composition(img)\n            }\n            \n            # åˆå¹¶ç»“æœ\n            result = {**basic_info, **content_analysis}\n            return result\n            \n    except Exception as e:\n        return {\n            \"type\": \"image\",\n            \"error\": f\"å›¾ç‰‡å†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n        }\n\ndef perform_ocr_on_image(img):\n    \"\"\"å¯¹å›¾ç‰‡è¿›è¡ŒOCRæ–‡å­—è¯†åˆ«\"\"\"\n    if not OCR_AVAILABLE:\n        return {\n            \"status\": \"error\",\n            \"message\": \"OCRåŠŸèƒ½æœªå®‰è£…ï¼Œè¯·å®‰è£…pytesseractåº“: pip install pytesseract\",\n            \"suggestion\": \"åŒæ—¶éœ€è¦å®‰è£…Tesseract OCRå¼•æ“\"\n        }\n    \n    try:\n        # é¢„å¤„ç†å›¾ç‰‡ä»¥æé«˜OCRå‡†ç¡®æ€§\n        img_processed = preprocess_image_for_ocr(img)\n        \n        # å°è¯•å¤šç§OCRé…ç½®\n        ocr_results = []\n        \n        # é…ç½®1ï¼šé»˜è®¤é…ç½®\n        try:\n            text_default = pytesseract.image_to_string(img_processed, lang='chi_sim+eng')\n            if text_default.strip():\n                ocr_results.append({\n                    \"config\": \"é»˜è®¤é…ç½®\",\n                    \"text\": text_default.strip(),\n                    \"confidence\": \"æ ‡å‡†\"\n                })\n        except Exception as e:\n            pass\n        \n        # é…ç½®2ï¼šåªè¯†åˆ«ä¸­æ–‡\n        try:\n            text_chinese = pytesseract.image_to_string(img_processed, lang='chi_sim')\n            if text_chinese.strip():\n                ocr_results.append({\n                    \"config\": \"ä¸­æ–‡è¯†åˆ«\",\n                    \"text\": text_chinese.strip(),\n                    \"confidence\": \"æ ‡å‡†\"\n                })\n        except Exception as e:\n            pass\n        \n        # é…ç½®3ï¼šåªè¯†åˆ«è‹±æ–‡\n        try:\n            text_english = pytesseract.image_to_string(img_processed, lang='eng')\n            if text_english.strip():\n                ocr_results.append({\n                    \"config\": \"è‹±æ–‡è¯†åˆ«\",\n                    \"text\": text_english.strip(),\n                    \"confidence\": \"æ ‡å‡†\"\n                })\n        except Exception as e:\n            pass\n        \n        # é…ç½®4ï¼šæ•°å­—è¯†åˆ«\n        try:\n            text_digits = pytesseract.image_to_string(img_processed, lang='eng', config='--psm 6 -c tessedit_char_whitelist=0123456789')\n            if text_digits.strip():\n                ocr_results.append({\n                    \"config\": \"æ•°å­—è¯†åˆ«\",\n                    \"text\": text_digits.strip(),\n                    \"confidence\": \"æ ‡å‡†\"\n                })\n        except Exception as e:\n            pass\n        \n        if ocr_results:\n            # é€‰æ‹©æœ€ä½³ç»“æœï¼ˆé€šå¸¸æ˜¯æœ€é•¿çš„æ–‡æœ¬ï¼‰\n            best_result = max(ocr_results, key=lambda x: len(x[\"text\"]))\n            \n            return {\n                \"status\": \"success\",\n                \"extracted_text\": best_result[\"text\"],\n                \"all_results\": ocr_results,\n                \"text_length\": len(best_result[\"text\"]),\n                \"word_count\": len(best_result[\"text\"].split()),\n                \"has_text\": True,\n                \"description\": f\"æˆåŠŸè¯†åˆ«åˆ°{len(best_result['text'])}ä¸ªå­—ç¬¦çš„æ–‡å­—å†…å®¹\"\n            }\n        else:\n            return {\n                \"status\": \"no_text\",\n                \"extracted_text\": \"\",\n                \"all_results\": [],\n                \"text_length\": 0,\n                \"word_count\": 0,\n                \"has_text\": False,\n                \"description\": \"æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡å­—å†…å®¹\"\n            }\n        \n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"OCRè¯†åˆ«å¤±è´¥: {str(e)}\",\n            \"extracted_text\": \"\",\n            \"all_results\": [],\n            \"text_length\": 0,\n            \"word_count\": 0,\n            \"has_text\": False\n        }\n\ndef preprocess_image_for_ocr(img):\n    \"\"\"é¢„å¤„ç†å›¾ç‰‡ä»¥æé«˜OCRå‡†ç¡®æ€§\"\"\"\n    try:\n        # è½¬æ¢ä¸ºRGBæ¨¡å¼\n        if img.mode != 'RGB':\n            img = img.convert('RGB')\n        \n        # è½¬æ¢ä¸ºç°åº¦å›¾\n        img_gray = img.convert('L')\n        \n        # å¢å¼ºå¯¹æ¯”åº¦\n        enhancer = ImageEnhance.Contrast(img_gray)\n        img_enhanced = enhancer.enhance(2.0)\n        \n        # å¢å¼ºé”åº¦\n        sharpness_enhancer = ImageEnhance.Sharpness(img_enhanced)\n        img_sharp = sharpness_enhancer.enhance(1.5)\n        \n        return img_sharp\n        \n    except Exception as e:\n        # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå›¾\n        return img\n\ndef analyze_document_content(file_path):\n    \"\"\"åˆ†ææ–‡æ¡£å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æå–ã€ä¸»é¢˜åˆ†æã€å…³é”®ä¿¡æ¯æå–ç­‰\"\"\"\n    extension = file_path.suffix.lower()\n    \n    if extension == '.pdf':\n        return analyze_pdf_content(file_path)\n    elif extension == '.txt':\n        return analyze_text_content(file_path)\n    elif extension == '.csv':\n        return analyze_csv_content(file_path)\n    elif extension == '.json':\n        return analyze_json_content(file_path)\n    else:\n        return {\n            \"type\": \"document\",\n            \"message\": f\"æš‚ä¸æ”¯æŒåˆ†æ {extension} æ ¼å¼çš„æ–‡æ¡£å†…å®¹\"\n        }\n\ndef analyze_image_scene(img):\n    \"\"\"åˆ†æå›¾ç‰‡åœºæ™¯\"\"\"\n    img_array = img.convert('L')\n    pixels = list(img_array.getdata())\n    avg_brightness = sum(pixels) / len(pixels)\n    \n    if avg_brightness > 200:\n        scene_type = \"æ˜äº®åœºæ™¯\"\n    elif avg_brightness > 100:\n        scene_type = \"æ­£å¸¸äº®åº¦\"\n    else:\n        scene_type = \"æš—å…‰åœºæ™¯\"\n    \n    return {\n        \"scene_type\": scene_type,\n        \"brightness_level\": round(avg_brightness, 2),\n        \"description\": f\"è¿™æ˜¯ä¸€å¼ {scene_type}çš„å›¾ç‰‡ï¼Œå¹³å‡äº®åº¦ä¸º{round(avg_brightness, 2)}\"\n    }\n\ndef detect_objects_in_image(img):\n    \"\"\"æ£€æµ‹å›¾ç‰‡ä¸­çš„ç‰©ä½“\"\"\"\n    img_array = img.convert('RGB')\n    pixels = list(img_array.getdata())\n    unique_colors = len(set(pixels))\n    \n    if unique_colors > 10000:\n        complexity = \"å¤æ‚\"\n        object_count_estimate = \"å¯èƒ½åŒ…å«å¤šä¸ªç‰©ä½“\"\n    elif unique_colors > 5000:\n        complexity = \"ä¸­ç­‰\"\n        object_count_estimate = \"å¯èƒ½åŒ…å«å‡ ä¸ªä¸»è¦ç‰©ä½“\"\n    else:\n        complexity = \"ç®€å•\"\n        object_count_estimate = \"å¯èƒ½åŒ…å«å°‘é‡ç‰©ä½“\"\n    \n    return {\n        \"complexity\": complexity,\n        \"unique_colors\": unique_colors,\n        \"object_count_estimate\": object_count_estimate,\n        \"description\": f\"å›¾ç‰‡å¤æ‚åº¦ä¸º{complexity}ï¼ŒåŒ…å«{unique_colors}ç§ä¸åŒé¢œè‰²\"\n    }\n\ndef extract_text_from_image(img):\n    \"\"\"ä»å›¾ç‰‡ä¸­æå–æ–‡å­—\"\"\"\n    img_array = img.convert('L')\n    pixels = list(img_array.getdata())\n    width, height = img.size\n    \n    # æ”¹è¿›çš„è¾¹ç¼˜å¯†åº¦è®¡ç®—\n    edge_density = calculate_edge_density_improved(pixels, width, height)\n    \n    # è®¡ç®—æ–‡å­—ç‰¹å¾\n    text_features = analyze_text_features(img)\n    \n    # ç»¼åˆåˆ¤æ–­æ–‡å­—å¯èƒ½æ€§\n    if edge_density > 0.25 or text_features['high_contrast_areas'] > 0.1:\n        text_likelihood = \"é«˜\"\n        description = \"å›¾ç‰‡å¾ˆå¯èƒ½åŒ…å«æ–‡å­—å†…å®¹\"\n    elif edge_density > 0.15 or text_features['high_contrast_areas'] > 0.05:\n        text_likelihood = \"ä¸­ç­‰\"\n        description = \"å›¾ç‰‡å¯èƒ½åŒ…å«å°‘é‡æ–‡å­—\"\n    else:\n        text_likelihood = \"ä½\"\n        description = \"å›¾ç‰‡å¯èƒ½ä¸åŒ…å«æ–‡å­—\"\n    \n    return {\n        \"text_likelihood\": text_likelihood,\n        \"edge_density\": round(edge_density, 3),\n        \"text_features\": text_features,\n        \"description\": description\n    }\n\ndef calculate_edge_density_improved(pixels, width, height):\n    \"\"\"æ”¹è¿›çš„è¾¹ç¼˜å¯†åº¦è®¡ç®—\"\"\"\n    edge_count = 0\n    total_pixels = len(pixels)\n    \n    # æ°´å¹³è¾¹ç¼˜æ£€æµ‹\n    for y in range(height):\n        for x in range(1, width):\n            idx = y * width + x\n            if abs(pixels[idx] - pixels[idx - 1]) > 25:\n                edge_count += 1\n    \n    # å‚ç›´è¾¹ç¼˜æ£€æµ‹\n    for y in range(1, height):\n        for x in range(width):\n            idx = y * width + x\n            if abs(pixels[idx] - pixels[idx - width]) > 25:\n                edge_count += 1\n    \n    return edge_count / (total_pixels * 2) if total_pixels > 0 else 0\n\ndef analyze_text_features(img):\n    \"\"\"åˆ†ææ–‡å­—ç‰¹å¾\"\"\"\n    img_array = img.convert('L')\n    pixels = list(img_array.getdata())\n    width, height = img.size\n    \n    # è®¡ç®—é«˜å¯¹æ¯”åº¦åŒºåŸŸ\n    high_contrast_pixels = 0\n    total_pixels = len(pixels)\n    \n    for i in range(1, len(pixels)):\n        if abs(pixels[i] - pixels[i-1]) > 50:\n            high_contrast_pixels += 1\n    \n    high_contrast_ratio = high_contrast_pixels / total_pixels if total_pixels > 0 else 0\n    \n    # è®¡ç®—äº®åº¦åˆ†å¸ƒ\n    brightness_values = [p for p in pixels]\n    avg_brightness = sum(brightness_values) / len(brightness_values) if brightness_values else 0\n    \n    # è®¡ç®—æ ‡å‡†å·®ï¼ˆç”¨äºåˆ¤æ–­å¯¹æ¯”åº¦ï¼‰\n    variance = sum((p - avg_brightness) ** 2 for p in brightness_values) / len(brightness_values) if brightness_values else 0\n    std_dev = variance ** 0.5\n    \n    return {\n        \"high_contrast_areas\": round(high_contrast_ratio, 3),\n        \"average_brightness\": round(avg_brightness, 2),\n        \"contrast_std_dev\": round(std_dev, 2),\n        \"description\": f\"é«˜å¯¹æ¯”åº¦åŒºåŸŸå æ¯”{round(high_contrast_ratio * 100, 1)}%ï¼Œå¹³å‡äº®åº¦{round(avg_brightness, 1)}ï¼Œå¯¹æ¯”åº¦æ ‡å‡†å·®{round(std_dev, 1)}\"\n    }\n\ndef analyze_image_colors(img):\n    \"\"\"åˆ†æå›¾ç‰‡é¢œè‰²ç‰¹å¾\"\"\"\n    img_array = img.convert('RGB')\n    pixels = list(img_array.getdata())\n    color_counts = {}\n    for pixel in pixels:\n        color_counts[pixel] = color_counts.get(pixel, 0) + 1\n    \n    sorted_colors = sorted(color_counts.items(), key=lambda x: x[1], reverse=True)\n    dominant_colors = sorted_colors[:5]\n    \n    total_pixels = len(pixels)\n    color_analysis = []\n    \n    for color, count in dominant_colors:\n        percentage = (count / total_pixels) * 100\n        color_analysis.append({\n            \"color\": f\"RGB{color}\",\n            \"percentage\": round(percentage, 2),\n            \"count\": count\n        })\n    \n    return {\n        \"dominant_colors\": color_analysis,\n        \"total_unique_colors\": len(color_counts),\n        \"description\": f\"å›¾ç‰‡åŒ…å«{len(color_counts)}ç§ä¸åŒé¢œè‰²ï¼Œä¸»è¦é¢œè‰²å æ¯”æœ€é«˜ä¸º{color_analysis[0]['percentage']}%\"\n    }\n\ndef analyze_image_composition(img):\n    \"\"\"åˆ†æå›¾ç‰‡æ„å›¾\"\"\"\n    width, height = img.size\n    aspect_ratio = width / height\n    \n    if abs(aspect_ratio - 1) < 0.1:\n        composition = \"æ­£æ–¹å½¢æ„å›¾\"\n    elif aspect_ratio > 1.5:\n        composition = \"æ¨ªå‘æ„å›¾\"\n    elif aspect_ratio < 0.7:\n        composition = \"çºµå‘æ„å›¾\"\n    else:\n        composition = \"æ ‡å‡†æ„å›¾\"\n    \n    if width * height > 8000000:\n        resolution_quality = \"é«˜åˆ†è¾¨ç‡\"\n    elif width * height > 2000000:\n        resolution_quality = \"ä¸­ç­‰åˆ†è¾¨ç‡\"\n    else:\n        resolution_quality = \"ä½åˆ†è¾¨ç‡\"\n    \n    return {\n        \"composition_type\": composition,\n        \"resolution_quality\": resolution_quality,\n        \"pixel_count\": width * height,\n        \"description\": f\"{composition}ï¼Œ{resolution_quality}ï¼Œæ€»åƒç´ {width * height:,}\"\n    }\n\ndef analyze_pdf_content(file_path):\n    \"\"\"åˆ†æPDFæ–‡æ¡£å†…å®¹\"\"\"\n    if not PDF_AVAILABLE:\n        return {\n            \"type\": \"pdf\",\n            \"error\": \"PyMuPDFåº“æœªå®‰è£…ï¼Œæ— æ³•åˆ†æPDFå†…å®¹\",\n            \"suggestion\": \"è¯·å®‰è£…PyMuPDFåº“: pip install PyMuPDF\"\n        }\n    \n    try:\n        doc = fitz.open(file_path)\n        full_text = \"\"\n        for page_num in range(len(doc)):\n            page = doc.load_page(page_num)\n            full_text += page.get_text()\n        \n        text_analysis = analyze_text_content_from_string(full_text)\n        \n        result = {\n            \"type\": \"pdf\",\n            \"page_count\": len(doc),\n            \"title\": doc.metadata.get(\"title\", \"æœªçŸ¥\"),\n            \"author\": doc.metadata.get(\"author\", \"æœªçŸ¥\"),\n            \"subject\": doc.metadata.get(\"subject\", \"æœªçŸ¥\"),\n            \"text_analysis\": text_analysis\n        }\n        \n        doc.close()\n        return result\n        \n    except Exception as e:\n        return {\n            \"type\": \"pdf\",\n            \"error\": f\"PDFå†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n        }\n\ndef analyze_text_content(file_path):\n    \"\"\"åˆ†ææ–‡æœ¬æ–‡ä»¶å†…å®¹\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        \n        return analyze_text_content_from_string(content)\n        \n    except Exception as e:\n        return {\n            \"type\": \"text\",\n            \"error\": f\"æ–‡æœ¬å†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n        }\n\ndef analyze_text_content_from_string(text):\n    \"\"\"åˆ†ææ–‡æœ¬å­—ç¬¦ä¸²å†…å®¹\"\"\"\n    if not text.strip():\n        return {\n            \"type\": \"text\",\n            \"message\": \"æ–‡ä»¶ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦\"\n        }\n    \n    lines = text.split('\\n')\n    words = text.split()\n    sentences = text.split('ã€‚')\n    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n    \n    if len(text) < 100:\n        text_type = \"çŸ­æ–‡æœ¬\"\n    elif len(text) < 1000:\n        text_type = \"ä¸­ç­‰æ–‡æœ¬\"\n    else:\n        text_type = \"é•¿æ–‡æœ¬\"\n    \n    chinese_chars = sum(1 for char in text if '\\u4e00' <= char <= '\\u9fff')\n    english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)\n    \n    if chinese_chars > english_chars:\n        language = \"ä¸­æ–‡\"\n    elif english_chars > chinese_chars:\n        language = \"è‹±æ–‡\"\n    else:\n        language = \"æ··åˆè¯­è¨€\"\n    \n    word_freq = {}\n    for word in words:\n        if len(word) > 1:\n            word_freq[word] = word_freq.get(word, 0) + 1\n    \n    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n    keywords = [word for word, count in sorted_words[:10] if count > 1]\n    \n    return {\n        \"type\": \"text\",\n        \"character_count\": len(text),\n        \"line_count\": len(lines),\n        \"word_count\": len(words),\n        \"sentence_count\": len(sentences),\n        \"avg_sentence_length\": round(avg_sentence_length, 2),\n        \"text_type\": text_type,\n        \"language\": language,\n        \"keywords\": keywords,\n        \"content_preview\": text[:500] + \"...\" if len(text) > 500 else text,\n        \"description\": f\"{text_type}ï¼Œ{language}ï¼ŒåŒ…å«{len(words)}ä¸ªè¯ï¼Œ{len(sentences)}ä¸ªå¥å­\"\n    }\n\ndef analyze_csv_content(file_path):\n    \"\"\"åˆ†æCSVæ–‡ä»¶å†…å®¹\"\"\"\n    if not PANDAS_AVAILABLE:\n        return {\n            \"type\": \"csv\",\n            \"error\": \"pandasåº“æœªå®‰è£…ï¼Œæ— æ³•åˆ†æCSVå†…å®¹\",\n            \"suggestion\": \"è¯·å®‰è£…pandasåº“: pip install pandas\"\n        }\n    \n    try:\n        df = pd.read_csv(file_path)\n        \n        basic_info = {\n            \"type\": \"csv\",\n            \"row_count\": len(df),\n            \"column_count\": len(df.columns),\n            \"columns\": df.columns.tolist()\n        }\n        \n        data_types = df.dtypes.to_dict()\n        numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n        text_columns = df.select_dtypes(include=['object']).columns.tolist()\n        \n        numeric_stats = {}\n        for col in numeric_columns:\n            numeric_stats[col] = {\n                \"mean\": float(df[col].mean()) if not df[col].isna().all() else None,\n                \"std\": float(df[col].mean()) if not df[col].isna().all() else None,\n                \"min\": float(df[col].min()) if not df[col].isna().all() else None,\n                \"max\": float(df[col].max()) if not df[col].isna().all() else None\n            }\n        \n        missing_values = df.isnull().sum().to_dict()\n        preview = df.head(5).to_dict()\n        \n        result = {\n            **basic_info,\n            \"data_types\": data_types,\n            \"numeric_columns\": numeric_columns,\n            \"text_columns\": text_columns,\n            \"numeric_stats\": numeric_stats,\n            \"missing_values\": missing_values,\n            \"preview\": preview,\n            \"description\": f\"CSVæ–‡ä»¶åŒ…å«{len(df)}è¡Œ{len(df.columns)}åˆ—æ•°æ®ï¼Œå…¶ä¸­{len(numeric_columns)}ä¸ªæ•°å€¼åˆ—ï¼Œ{len(text_columns)}ä¸ªæ–‡æœ¬åˆ—\"\n        }\n        \n        return result\n        \n    except Exception as e:\n        return {\n            \"type\": \"csv\",\n            \"error\": f\"CSVå†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n        }\n\ndef analyze_json_content(file_path):\n    \"\"\"åˆ†æJSONæ–‡ä»¶å†…å®¹\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        basic_info = {\n            \"type\": \"json\",\n            \"data_type\": type(data).__name__\n        }\n        \n        structure_analysis = analyze_json_structure(data)\n        content_preview = json.dumps(data, ensure_ascii=False, indent=2)[:500] + \"...\" if len(json.dumps(data)) > 500 else json.dumps(data, ensure_ascii=False, indent=2)\n        \n        result = {\n            **basic_info,\n            \"structure_analysis\": structure_analysis,\n            \"content_preview\": content_preview,\n            \"description\": f\"JSONæ–‡ä»¶åŒ…å«{structure_analysis.get('total_elements', 0)}ä¸ªå…ƒç´ ï¼Œæ•°æ®ç»“æ„ä¸º{type(data).__name__}\"\n        }\n        \n        return result\n        \n    except Exception as e:\n        return {\n            \"type\": \"json\",\n            \"error\": f\"JSONå†…å®¹åˆ†æå¤±è´¥: {str(e)}\"\n        }\n\ndef get_basic_file_info(file_path):\n    \"\"\"è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯\"\"\"\n    stat = file_path.stat()\n    file_hash = calculate_file_hash(file_path)\n    mime_type, _ = mimetypes.guess_type(str(file_path))\n    \n    return {\n        \"file_name\": file_path.name,\n        \"file_path\": str(file_path.absolute()),\n        \"file_size\": stat.st_size,\n        \"file_size_human\": format_file_size(stat.st_size),\n        \"file_extension\": file_path.suffix.lower(),\n        \"mime_type\": mime_type or \"unknown\",\n        \"created_time\": datetime.datetime.fromtimestamp(stat.st_ctime).strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"modified_time\": datetime.datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"file_hash\": file_hash,\n        \"is_readable\": os.access(file_path, os.R_OK),\n        \"is_writable\": os.access(file_path, os.W_OK)\n    }\n\ndef is_image_file(file_path):\n    \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶\"\"\"\n    supported_formats = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n    return file_path.suffix.lower() in supported_formats\n\ndef is_document_file(file_path):\n    \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æ¡£æ–‡ä»¶\"\"\"\n    supported_formats = {'.pdf', '.txt', '.doc', '.docx', '.csv', '.json', '.xml'}\n    return file_path.suffix.lower() in supported_formats\n\ndef calculate_file_hash(file_path):\n    \"\"\"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼\"\"\"\n    try:\n        hash_md5 = hashlib.md5()\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    except:\n        return \"è®¡ç®—å¤±è´¥\"\n\ndef format_file_size(size_bytes):\n    \"\"\"æ ¼å¼åŒ–æ–‡ä»¶å¤§å°\"\"\"\n    if size_bytes == 0:\n        return \"0B\"\n    \n    size_names = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n    i = 0\n    while size_bytes >= 1024 and i < len(size_names) - 1:\n        size_bytes /= 1024.0\n        i += 1\n    \n    return f\"{size_bytes:.2f} {size_names[i]}\"\n\ndef get_color_depth(mode):\n    \"\"\"è·å–é¢œè‰²æ·±åº¦\"\"\"\n    depth_map = {\n        \"1\": \"1ä½ï¼ˆé»‘ç™½ï¼‰\",\n        \"L\": \"8ä½ï¼ˆç°åº¦ï¼‰\",\n        \"P\": \"8ä½ï¼ˆè°ƒè‰²æ¿ï¼‰\",\n        \"RGB\": \"24ä½ï¼ˆRGBï¼‰\",\n        \"RGBA\": \"32ä½ï¼ˆRGBAï¼‰\",\n        \"CMYK\": \"32ä½ï¼ˆCMYKï¼‰\",\n        \"YCbCr\": \"24ä½ï¼ˆYCbCrï¼‰\",\n        \"LAB\": \"24ä½ï¼ˆLABï¼‰\",\n        \"HSV\": \"24ä½ï¼ˆHSVï¼‰\"\n    }\n    return depth_map.get(mode, f\"æœªçŸ¥æ¨¡å¼: {mode}\")\n\ndef calculate_edge_density(pixels, width, height):\n    \"\"\"è®¡ç®—è¾¹ç¼˜å¯†åº¦ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åŒ…å«æ–‡å­—ï¼‰\"\"\"\n    edge_count = 0\n    total_pixels = len(pixels)\n    \n    for i in range(1, len(pixels)):\n        if abs(pixels[i] - pixels[i-1]) > 30:\n            edge_count += 1\n    \n    return edge_count / total_pixels if total_pixels > 0 else 0\n\ndef analyze_json_structure(data, max_depth=3):\n    \"\"\"åˆ†æJSONç»“æ„\"\"\"\n    def analyze_recursive(obj, depth=0):\n        if depth > max_depth:\n            return \"æ·±åº¦é™åˆ¶\"\n        \n        if isinstance(obj, dict):\n            return {\n                \"type\": \"object\",\n                \"keys\": list(obj.keys()),\n                \"key_count\": len(obj),\n                \"sample_values\": {k: analyze_recursive(v, depth + 1) for k, v in list(obj.items())[:3]}\n            }\n        elif isinstance(obj, list):\n            return {\n                \"type\": \"array\",\n                \"length\": len(obj),\n                \"sample_items\": [analyze_recursive(item, depth + 1) for item in obj[:3]]\n            }\n        else:\n            return {\n                \"type\": type(obj).__name__,\n                \"value\": str(obj)[:100] if obj else None\n            }\n    \n    structure = analyze_recursive(data)\n    \n    def count_elements(obj):\n        if isinstance(obj, (dict, list)):\n            return 1 + sum(count_elements(item) for item in (obj.values() if isinstance(obj, dict) else obj))\n        return 1\n    \n    structure[\"total_elements\"] = count_elements(data)\n    return structure\n\ndef upload_and_analyze_file(file_path):\n    \"\"\"ä¸Šä¼ å¹¶æ·±åº¦åˆ†ææ–‡ä»¶\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            return f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\"\n        \n        analysis_result = analyze_file_content(file_path)\n        \n        result = f\"ğŸ” æ™ºèƒ½æ–‡ä»¶åˆ†æç»“æœ\\n\"\n        result += f\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\"\n        result += f\"æ–‡ä»¶è·¯å¾„: {file_path}\\n\"\n        result += f\"åˆ†ææ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        result += f\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\"\n        result += analysis_result\n        \n        return result\n        \n    except Exception as e:\n        return f\"æ–‡ä»¶ä¸Šä¼ åˆ†æå¤±è´¥: {str(e)}\"",
    "type": "custom"
  }
}